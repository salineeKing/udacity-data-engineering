{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OFLC H-1B Program Data and Job Board Sites\n",
    "## Data Engineering Capstone Project\n",
    "### Author - Salinee Kingbaisomboon\n",
    "\n",
    "#### Project Summary\n",
    "The primary purpose of this project is to prepare the data structure in order to be viewed & analysed by many parties such as H1B job seekers, international students, HR recruiters and public who are interested to incorporate this data to their interest, for example, **planning for in-demand career paths**, **H1B workers trend prediction**, **H1B processing time analysis & forcast** and so on.\n",
    "\n",
    "This project will be mainly focusing on the end-to-end **Data Pipelines Process** starting from <i>integrate data from various resources</i>, perform an <i>ETL (Extract, Transform and Load) on top of this big datasets</i> into a new form of table schema. Then create **parquet files** from those new tables and finally, write these files into a <i>data lake</i> **Amazon S3**.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "The H-1B is a visa in the United States under the Immigration and Nationality Act, section 101 that allows U.S. employers to temporarily employ foreign workers in specialty occupations. A specialty occupation requires the application of specialized knowledge and a bachelor's degree or the equivalent of work experience. The duration of stay is three years, extendable to six years; after which the visa holder may need to reapply. Laws limit the number of H-1B visas that are issued each year: 188,100 new and initial H-1B visas were issued in 2019.Employers must generally withhold Social Security and Medicare taxes from the wages paid to employees in H-1B status.\n",
    "\n",
    "The H-1B visa has its roots in the H1 visa of the Immigration and Nationality Act of 1952; the split between H-1A (for nurses) and H-1B was created by the Immigration Act of 1990. 65,000 H-1B visas were made available each fiscal year, out of which employers could apply through Labor Condition Applications. Additional modifications to H1-B rules were made by legislation in 1998, 2000, in 2003 for Singapore and Chile, in the H-1B Visa Reform Act of 2004, 2008, and 2009. United States Citizenship and Immigration Services has modified the rules in the years since then.<i>[1]</i>\n",
    "\n",
    "<i>[1] Source: https://en.wikipedia.org/wiki/H-1B_visa</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Important Role that H-1B Workers Play in the U.S. Economy\n",
    "\n",
    "Foreign workers fill a critical need in the U.S. labor market—particularly in the Science, Technology, Engineering, and Math (STEM) fields.\n",
    "\n",
    "<img src=\"https://www.urban.org/sites/default/files/styles/feature2_full_hero/public/feature2/jobs-feature-header-1700x700.png?itok=8lB15_K3\" width=\"500\"/>\n",
    "<center><i>Source: https://www.urban.org/features/how-government-jobs-programs-could-boost-employment</i></center>\n",
    "\n",
    "The United States has a successful economy system. Foreign-born workers of all types and skills, from every corner of the globe, have joined with american-born workers to build it. Skilled immigrants’ contributions to the U.S. economy help create new jobs and new opportunities for economic expansion. Indeed, H-1B workers **positively** impact the U.S. economy and the employment opportunities of american-born workers.\n",
    "\n",
    "The skills that H-1B workers bring with them can be critical in responding to national emergencies. For instance, over the past decade (FY 2010-FY 2019), eight companies that are currently trying to develop a coronavirus vaccine—Gilead Sciences, Moderna Therapeutics, GlaxoSmithKline, Inovio, Johnson and Johnson Pharmaceuticals, Regeneron, Vir Therapeutics, and Sanofi—received approvals for 3,310 biochemists, biophysicists, chemists, and other scientists through the H-1B program.<i>[2]</i>\n",
    "\n",
    "<i>[2] Source: https://www.americanimmigrationcouncil.org/research/h1b-visa-program-fact-sheet#:~:text=Skilled%20immigrants'%20contributions%20to%20the,opportunities%20of%20native%2Dborn%20workers.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Import all necessary libraries and perform the installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, lit\n",
    "from pyspark.sql.functions import date_format, to_date, unix_timestamp, from_unixtime\n",
    "from pyspark.sql.types import FloatType, IntegerType, TimestampType, StringType, LongType, DateType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scope  \n",
    "**<u><font color=blue>Plan for the Project:</font></u>**\n",
    "1. Gathering three datasets which will be used to construct a new data model.\n",
    "    - 1.1 **H1B** dataset which will be used as primary data source for the **fact table**.\n",
    "    - 1.2 Two of the **job posting** datasets which will be used to provide data source to the **dimension tables**. Since we want to provide the in-demand or in-trend jobs for the current markets and trying to related them back to the **H1B** jobs that employers in the U.S. are hiring.\n",
    "2. After that, we will start exploring all the datasets and perform some data wrangling (discovering, cleaning, enriching, validating and such).\n",
    "3. Design the new database schema which will represent as the new data sources for users which can help them answer some questions, for example:\n",
    "    - 3.1 What kind of **job title**, **industry** are in-demand for hiring.\n",
    "    - 3.2 What **cities** or **states** are thoes jobs?\n",
    "    - 3.1 How long does it take for **H1B** application processes from the submitted date until the decision date.\n",
    "    \n",
    "    Thoese questions will help many people (such as international students) to make their decisions to choose their program of study, where should they looking for a job after graduating and such.\n",
    "4. Create a python scripts to perform **Data Pipelines** processes.\n",
    "    - 4.1 The main goal is to create a very simple **Data Lake** on **AWS S3** by gathering various datasets and perform the **ETL** on those raw data and convert them into a new schema structure with a new form of entities and relationship.\n",
    "    - 4.2 We will use **Spark** (local mode) to ingest all data, extract and transform them into the designated fact & dimension tables.\n",
    "    - 4.3 Then, we will generated **parquet files** based on each tables. All thoses files will be written to **AWS S3** via  **Spark session**.\n",
    "5. The end result would be the new completely datasets reside on data lake which will be available to accesses for any parties who are intereted.\n",
    "\n",
    "**<u><font color=blue>Data used in this Project:</font></u>**\n",
    "\n",
    "There are three datasets used in this project.\n",
    "\n",
    "All of the datasets used in this project are from <a href=\"https://www.kaggle.com\" target=\"_blank\">Kaggle</a> which is a subsidiary of <a href=\"https://www.google.com\" target=\"_blank\">Google LLC</a>, is an online community of data scientists and machine learning practitioners.<i>[3]</i>\n",
    "\n",
    "**<u><font color=blue>End Solution:</font></u>**\n",
    "This project will be a very simple **Data Pipeline Solutions** which in the end will be served for querying through a **Data Lake Storage on AWS** via **S3** bucket.\n",
    "\n",
    "**<u><font color=blue>Tools used in this Projects:</font></u>**\n",
    "- AWS regular account\n",
    "- AWS S3\n",
    "- Jupyter Notebook\n",
    "- Spark (built-in local mode)\n",
    "\n",
    "<i>[3] Source: https://en.wikipedia.org/wiki/Kaggle.</i>\n",
    "\n",
    "<i>[4] Source: https://en.wikipedia.org/wiki/Star_schema#:~:text=The%20star%20schema%20is%20an,it%20representing%20the%20star's%20points..</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe and Gather Data\n",
    "Below are three of the datasets I'll used in this project. \n",
    "\n",
    "1. **OFLC H-1B Program Data (2011-2018)**\n",
    "   - Source: <a href=\"https://www.kaggle.com/rakeshchintha/oflc-h1b-data\" target=\"_blank\">Kaggle</a>\n",
    "   - File Name: **h1b_data_fy2011_fy2018_20190401.csv**\n",
    "   - File Extension: csv\n",
    "   - This dataset contains 8 years worth of H-1B LCA petition data from 2011-2018, with more than **4 million records**.<br />\n",
    "       **Available Fields**:\n",
    "       - Fiscal Year\n",
    "       - Case Status\n",
    "       - Case Submitted\n",
    "       - Decision Date\n",
    "       - Employer Name\n",
    "       - Employer City\n",
    "       - Employer State\n",
    "       - Employer Zip\n",
    "       - Job Title\n",
    "       - SOC Code <i>(More details: https://en.wikipedia.org/wiki/Standard_Occupational_Classification_System)</i>\n",
    "       - SOC Name\n",
    "       - NAIC S Code <i>(More details: https://en.wikipedia.org/wiki/North_American_Industry_Classification_System)</i>\n",
    "       - Fulltime Position\n",
    "       - Prevailing Wage\n",
    "       - Prevailing Wage Unit\n",
    "       - Wage From\n",
    "       - Wage To\n",
    "       - Wage Unit\n",
    "       - Work City\n",
    "       - Work State\n",
    "       - Work Zip\n",
    "2. **Indeed Job Posting Dataset**\n",
    "   - Source: <a href=\"https://www.kaggle.com/promptcloud/indeed-job-posting-dataset\" target=\"_blank\">Kaggle</a>\n",
    "   - File Name: **marketing_sample_for_trulia_com-real_estate__20190901_20191031__30k_data.csv**\n",
    "   - File Extension: csv\n",
    "   - This dataset contains around 30K records of jobs posting from date range from 1st Aug 2019 to 31st Oct 2019 on https://www.indeed.com/.<br />\n",
    "       **Available Fields**:\n",
    "       - Job Title\n",
    "       - Job Description\n",
    "       - Job Type\n",
    "       - Categories\n",
    "       - Location\n",
    "       - City\n",
    "       - State\n",
    "       - Country\n",
    "       - Zip Code\n",
    "       - Address\n",
    "       - Salary From\n",
    "       - Salary To\n",
    "       - Salary Period\n",
    "       - Apply Url\n",
    "       - Apply Email\n",
    "       - Employees\n",
    "       - Industry\n",
    "       - Company Name\n",
    "       - Employer Email\n",
    "       - Employer Website\n",
    "       - Employer Phone\n",
    "       - Employer Logo\n",
    "       - Company description\n",
    "       - Employer Location\n",
    "       - Employer City\n",
    "       - Employer State\n",
    "       - Employer Country\n",
    "       - Employer Zip Code\n",
    "       - Uniq Id\n",
    "       - Crawl Timestamp \n",
    "3. **Job Posting On Amazon Jobs**\n",
    "   - Source: <a href=\"https://www.kaggle.com/promptcloud/job-posting-on-amazon-jobs\" target=\"_blank\">Kaggle</a>\n",
    "   - File Name: **amazon_com-jobs__20190901_20191231_sample.xml**\n",
    "   - File Extension: xml\n",
    "   - This dataset contains around 30K records of jobs posting from date range from 1st Sep 2019 to 31st December 2019 on https://amazon.com/.<br />\n",
    "       **Available Fields**:\n",
    "       - Unique ID\n",
    "       - Crawl TimeStamp\n",
    "       - Job URL\n",
    "       - Title\n",
    "       - Description\n",
    "       - Location\n",
    "       - Category\n",
    "       - Job Id\n",
    "       - Company Name\n",
    "   - Note: The file extension is xml and we interest only the **record** tag and its children."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "In this step, we will explore each of our datasets using **Pandas** libraries in order to identify data quality issues. We will perform four tasks:\n",
    "1. Explore each datasets\n",
    "2. Identify missing values\n",
    "3. Identify duplicate data\n",
    "4. Perform data cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 OFLC H-1B Program Data (2011-2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>case_status</th>\n",
       "      <th>case_submitted</th>\n",
       "      <th>decision_date</th>\n",
       "      <th>emp_name</th>\n",
       "      <th>emp_city</th>\n",
       "      <th>emp_state</th>\n",
       "      <th>emp_zip</th>\n",
       "      <th>job_title</th>\n",
       "      <th>soc_code</th>\n",
       "      <th>...</th>\n",
       "      <th>naics_code</th>\n",
       "      <th>full_time_position</th>\n",
       "      <th>prevailing_wage</th>\n",
       "      <th>pw_unit</th>\n",
       "      <th>wage_from</th>\n",
       "      <th>wage_to</th>\n",
       "      <th>wage_unit</th>\n",
       "      <th>work_city</th>\n",
       "      <th>work_state</th>\n",
       "      <th>work_zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011</td>\n",
       "      <td>C</td>\n",
       "      <td>8/3/2011</td>\n",
       "      <td>8/9/2011</td>\n",
       "      <td>WILLISTON NORTHAMPTON SCHOOL</td>\n",
       "      <td>EASTHAMPTON</td>\n",
       "      <td>MA</td>\n",
       "      <td>1027.0</td>\n",
       "      <td>CHINESE TEACHER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>611110</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23350.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>40400.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>EASTHAMPTON</td>\n",
       "      <td>MA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011</td>\n",
       "      <td>C</td>\n",
       "      <td>1/11/2011</td>\n",
       "      <td>1/18/2011</td>\n",
       "      <td>NYFIX, INC.</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>NY</td>\n",
       "      <td>10005.0</td>\n",
       "      <td>PROJECT MANAGER</td>\n",
       "      <td>15-1031</td>\n",
       "      <td>...</td>\n",
       "      <td>5415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101088.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>NY</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011</td>\n",
       "      <td>CW</td>\n",
       "      <td>4/21/2011</td>\n",
       "      <td>4/27/2011</td>\n",
       "      <td>TGS-NOPEC GEOPHYSICAL COMPANY</td>\n",
       "      <td>HOUSTON</td>\n",
       "      <td>TX</td>\n",
       "      <td>77042.0</td>\n",
       "      <td>PRINCIPAL TRAINER / DEVELOPMENT ANALYST</td>\n",
       "      <td>15-1031</td>\n",
       "      <td>...</td>\n",
       "      <td>541360</td>\n",
       "      <td>1.0</td>\n",
       "      <td>77480.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>87500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>HOUSTON</td>\n",
       "      <td>TX</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011</td>\n",
       "      <td>C</td>\n",
       "      <td>4/19/2011</td>\n",
       "      <td>4/25/2011</td>\n",
       "      <td>AFREN USA, INC.</td>\n",
       "      <td>THE WOODLANDS</td>\n",
       "      <td>TX</td>\n",
       "      <td>77380.0</td>\n",
       "      <td>DRILLING MANAGER</td>\n",
       "      <td>11-9041</td>\n",
       "      <td>...</td>\n",
       "      <td>211111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>165506.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>233500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>THE WOODLANDS</td>\n",
       "      <td>TX</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011</td>\n",
       "      <td>C</td>\n",
       "      <td>4/4/2011</td>\n",
       "      <td>4/8/2011</td>\n",
       "      <td>BA-INSIGHT, LLC</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>NY</td>\n",
       "      <td>10165.0</td>\n",
       "      <td>TECHNICAL SUPPORT ENGINEER</td>\n",
       "      <td>15-1041</td>\n",
       "      <td>...</td>\n",
       "      <td>541519</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62358.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>68000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>BOSTON</td>\n",
       "      <td>MA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   fiscal_year case_status case_submitted decision_date  \\\n",
       "0         2011           C       8/3/2011      8/9/2011   \n",
       "1         2011           C      1/11/2011     1/18/2011   \n",
       "2         2011          CW      4/21/2011     4/27/2011   \n",
       "3         2011           C      4/19/2011     4/25/2011   \n",
       "4         2011           C       4/4/2011      4/8/2011   \n",
       "\n",
       "                        emp_name       emp_city emp_state  emp_zip  \\\n",
       "0   WILLISTON NORTHAMPTON SCHOOL    EASTHAMPTON        MA   1027.0   \n",
       "1                    NYFIX, INC.       NEW YORK        NY  10005.0   \n",
       "2  TGS-NOPEC GEOPHYSICAL COMPANY        HOUSTON        TX  77042.0   \n",
       "3                AFREN USA, INC.  THE WOODLANDS        TX  77380.0   \n",
       "4                BA-INSIGHT, LLC       NEW YORK        NY  10165.0   \n",
       "\n",
       "                                 job_title soc_code  ... naics_code  \\\n",
       "0                          CHINESE TEACHER      NaN  ...     611110   \n",
       "1                          PROJECT MANAGER  15-1031  ...       5415   \n",
       "2  PRINCIPAL TRAINER / DEVELOPMENT ANALYST  15-1031  ...     541360   \n",
       "3                         DRILLING MANAGER  11-9041  ...     211111   \n",
       "4               TECHNICAL SUPPORT ENGINEER  15-1041  ...     541519   \n",
       "\n",
       "  full_time_position  prevailing_wage  pw_unit wage_from  wage_to  wage_unit  \\\n",
       "0                1.0          23350.0        Y   40400.0      NaN          Y   \n",
       "1                1.0         101088.0        Y  150000.0      NaN          Y   \n",
       "2                1.0          77480.0        Y   87500.0      NaN          Y   \n",
       "3                1.0         165506.0        Y  233500.0      NaN          Y   \n",
       "4                1.0          62358.0        Y   68000.0      NaN          Y   \n",
       "\n",
       "       work_city work_state work_zip  \n",
       "0    EASTHAMPTON         MA      NaN  \n",
       "1       NEW YORK         NY      NaN  \n",
       "2        HOUSTON         TX      NaN  \n",
       "3  THE WOODLANDS         TX      NaN  \n",
       "4         BOSTON         MA      NaN  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "h1b_pandas_df = pd.read_csv('h1b_data_fy2011_fy2018_20190401.csv', engine='python')\n",
    "h1b_pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1B Dataset Shape:(4192087, 21)\n"
     ]
    }
   ],
   "source": [
    "print(f'H1B Dataset Shape:{h1b_pandas_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Indeed Job Posting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Job Description</th>\n",
       "      <th>Job Type</th>\n",
       "      <th>Categories</th>\n",
       "      <th>Location</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Address</th>\n",
       "      <th>...</th>\n",
       "      <th>Employer Phone</th>\n",
       "      <th>Employer Logo</th>\n",
       "      <th>Companydescription</th>\n",
       "      <th>Employer Location</th>\n",
       "      <th>Employer City</th>\n",
       "      <th>Employer State</th>\n",
       "      <th>Employer Country</th>\n",
       "      <th>Employer Zip Code</th>\n",
       "      <th>Uniq Id</th>\n",
       "      <th>Crawl Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shift Manager</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mission Hills, CA 91345</td>\n",
       "      <td>Mission Hills</td>\n",
       "      <td>CA</td>\n",
       "      <td>United States</td>\n",
       "      <td>91345</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://d2q79iu7y748jz.cloudfront.net/s/_squar...</td>\n",
       "      <td>Del Taco is an American quick service restaura...</td>\n",
       "      <td>Mission Hills, CA 91345</td>\n",
       "      <td>Mission Hills</td>\n",
       "      <td>CA</td>\n",
       "      <td>United States</td>\n",
       "      <td>91345</td>\n",
       "      <td>511f9a53920f4641d701d51d3589349f</td>\n",
       "      <td>2019-08-24 09:13:18 +0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Operations Support Manager</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Atlanta, GA 30342</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>GA</td>\n",
       "      <td>United States</td>\n",
       "      <td>30342</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://d2q79iu7y748jz.cloudfront.net/s/_logo/...</td>\n",
       "      <td>Based in Atlanta, FOCUS Brands Inc. is an inno...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4955daf0a3facbe2acb6c429ba394e6d</td>\n",
       "      <td>2019-09-19 08:16:55 +0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Product Manager - Data</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vibes Corp. reputation was built and establish...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a0e0d12df1571962b785f17f43ceae12</td>\n",
       "      <td>2019-09-18 02:13:10 +0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Part-Time Office Concierge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Festus, MO</td>\n",
       "      <td>Festus</td>\n",
       "      <td>MO</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56e411fd731f76ac916bf4fb169250e9</td>\n",
       "      <td>2019-10-24 16:39:13 +0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Print &amp; Marketing Associate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cedar Rapids, IA 52404</td>\n",
       "      <td>Cedar Rapids</td>\n",
       "      <td>IA</td>\n",
       "      <td>United States</td>\n",
       "      <td>52404</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://d2q79iu7y748jz.cloudfront.net/s/_logo/...</td>\n",
       "      <td>Staples is The Worklife Fulfillment Company, h...</td>\n",
       "      <td>Cedar Rapids, IA 52404</td>\n",
       "      <td>Cedar Rapids</td>\n",
       "      <td>IA</td>\n",
       "      <td>United States</td>\n",
       "      <td>52404</td>\n",
       "      <td>3fff5c0ad6981bf4bff6260bd5feab63</td>\n",
       "      <td>2019-08-24 22:29:10 +0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Job Title  Job Description  Job Type  Categories  \\\n",
       "0                  Shift Manager              NaN       NaN         NaN   \n",
       "1     Operations Support Manager              NaN       NaN         NaN   \n",
       "2  Senior Product Manager - Data              NaN       NaN         NaN   \n",
       "3     Part-Time Office Concierge              NaN       NaN         NaN   \n",
       "4    Print & Marketing Associate              NaN       NaN         NaN   \n",
       "\n",
       "                  Location           City State        Country Zip Code  \\\n",
       "0  Mission Hills, CA 91345  Mission Hills    CA  United States    91345   \n",
       "1        Atlanta, GA 30342        Atlanta    GA  United States    30342   \n",
       "2              Chicago, IL        Chicago    IL  United States      NaN   \n",
       "3               Festus, MO         Festus    MO  United States      NaN   \n",
       "4   Cedar Rapids, IA 52404   Cedar Rapids    IA  United States    52404   \n",
       "\n",
       "   Address  ...  Employer Phone  \\\n",
       "0      NaN  ...             NaN   \n",
       "1      NaN  ...             NaN   \n",
       "2      NaN  ...             NaN   \n",
       "3      NaN  ...             NaN   \n",
       "4      NaN  ...             NaN   \n",
       "\n",
       "                                       Employer Logo  \\\n",
       "0  https://d2q79iu7y748jz.cloudfront.net/s/_squar...   \n",
       "1  https://d2q79iu7y748jz.cloudfront.net/s/_logo/...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  https://d2q79iu7y748jz.cloudfront.net/s/_logo/...   \n",
       "\n",
       "                                  Companydescription        Employer Location  \\\n",
       "0  Del Taco is an American quick service restaura...  Mission Hills, CA 91345   \n",
       "1  Based in Atlanta, FOCUS Brands Inc. is an inno...                      NaN   \n",
       "2  Vibes Corp. reputation was built and establish...                      NaN   \n",
       "3                                                NaN                      NaN   \n",
       "4  Staples is The Worklife Fulfillment Company, h...   Cedar Rapids, IA 52404   \n",
       "\n",
       "   Employer City  Employer State  Employer Country Employer Zip Code  \\\n",
       "0  Mission Hills              CA     United States             91345   \n",
       "1            NaN             NaN     United States               NaN   \n",
       "2            NaN             NaN     United States               NaN   \n",
       "3            NaN             NaN     United States               NaN   \n",
       "4   Cedar Rapids              IA     United States             52404   \n",
       "\n",
       "                            Uniq Id            Crawl Timestamp  \n",
       "0  511f9a53920f4641d701d51d3589349f  2019-08-24 09:13:18 +0000  \n",
       "1  4955daf0a3facbe2acb6c429ba394e6d  2019-09-19 08:16:55 +0000  \n",
       "2  a0e0d12df1571962b785f17f43ceae12  2019-09-18 02:13:10 +0000  \n",
       "3  56e411fd731f76ac916bf4fb169250e9  2019-10-24 16:39:13 +0000  \n",
       "4  3fff5c0ad6981bf4bff6260bd5feab63  2019-08-24 22:29:10 +0000  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "indeed_jobs_pandas_df = pd.read_csv('marketing_sample_for_trulia_com-real_estate__20190901_20191031__30k_data.csv')\n",
    "indeed_jobs_pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indeed Jobs Posting Dataset Shape:(30002, 30)\n"
     ]
    }
   ],
   "source": [
    "print(f'Indeed Jobs Posting Dataset Shape:{indeed_jobs_pandas_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3 Job Posting On Amazon Jobs\n",
    "Note: Since this file is in a xml format, we need to parse it and convert the results into the pandas dataframe. I use **xml.etree.ElementTree** (https://docs.python.org/3/library/xml.etree.elementtree.html) from python library to perform this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "xtree = et.parse(\"amazon_com-jobs__20190901_20191231_sample.xml\")\n",
    "xroot = xtree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniq_id</th>\n",
       "      <th>crawl_timestamp</th>\n",
       "      <th>job_url</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>category</th>\n",
       "      <th>job_id</th>\n",
       "      <th>company_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>850f0199e2ce9d1d51ee5e2e11318d94</td>\n",
       "      <td>2019-11-15 00:15:23 +0000</td>\n",
       "      <td>https://www.amazon.jobs/en/jobs/986897/softwar...</td>\n",
       "      <td>Software Development Engineer I</td>\n",
       "      <td>Amazon’s Global Logistics Technology team is c...</td>\n",
       "      <td>US, WA, Seattle</td>\n",
       "      <td>Software Development</td>\n",
       "      <td>986897</td>\n",
       "      <td>Amazon.com Services, Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9215700a26684c9dca1ad77718f33ff2</td>\n",
       "      <td>2019-11-15 00:15:23 +0000</td>\n",
       "      <td>https://www.amazon.jobs/en/jobs/986896/busines...</td>\n",
       "      <td>Business Intelligence Engineer</td>\n",
       "      <td>Amazon Advertising is dedicated to driving mea...</td>\n",
       "      <td>US, CA, Palo Alto</td>\n",
       "      <td>Business Intelligence</td>\n",
       "      <td>986896</td>\n",
       "      <td>Amazon.com Services, Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14b00d40ced925890eb4eca458040133</td>\n",
       "      <td>2019-11-15 00:15:23 +0000</td>\n",
       "      <td>https://www.amazon.jobs/en/jobs/986894/senior-...</td>\n",
       "      <td>Senior Solution Design Manager</td>\n",
       "      <td>The Amazon Cross Border Exports team is lookin...</td>\n",
       "      <td>US, WA, Bellevue</td>\n",
       "      <td>Supply Chain/Transportation Management</td>\n",
       "      <td>986894</td>\n",
       "      <td>Amazon.com Services, Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0c96a59282dc24bbc546bf31d42825d5</td>\n",
       "      <td>2019-11-15 00:15:23 +0000</td>\n",
       "      <td>https://www.amazon.jobs/en/jobs/986893/instock...</td>\n",
       "      <td>Instock Manager</td>\n",
       "      <td>The AmazonFresh and Prime Now organizations ar...</td>\n",
       "      <td>US, WA, Seattle</td>\n",
       "      <td>Buying, Planning, &amp; Instock Management</td>\n",
       "      <td>986893</td>\n",
       "      <td>Amazon.com Services, Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fff278b8672d6999631c5517c081fc4d</td>\n",
       "      <td>2019-11-15 00:15:23 +0000</td>\n",
       "      <td>https://www.amazon.jobs/en/jobs/986891/shift-m...</td>\n",
       "      <td>Shift Manager, Logistics</td>\n",
       "      <td>Do you want to work hard, have fun and make hi...</td>\n",
       "      <td>CA, AB, Calgary</td>\n",
       "      <td>Fulfillment &amp; Operations Management</td>\n",
       "      <td>986891</td>\n",
       "      <td>AMZN CAN Fulfillment Svcs, ULC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            uniq_id            crawl_timestamp  \\\n",
       "0  850f0199e2ce9d1d51ee5e2e11318d94  2019-11-15 00:15:23 +0000   \n",
       "1  9215700a26684c9dca1ad77718f33ff2  2019-11-15 00:15:23 +0000   \n",
       "2  14b00d40ced925890eb4eca458040133  2019-11-15 00:15:23 +0000   \n",
       "3  0c96a59282dc24bbc546bf31d42825d5  2019-11-15 00:15:23 +0000   \n",
       "4  fff278b8672d6999631c5517c081fc4d  2019-11-15 00:15:23 +0000   \n",
       "\n",
       "                                             job_url  \\\n",
       "0  https://www.amazon.jobs/en/jobs/986897/softwar...   \n",
       "1  https://www.amazon.jobs/en/jobs/986896/busines...   \n",
       "2  https://www.amazon.jobs/en/jobs/986894/senior-...   \n",
       "3  https://www.amazon.jobs/en/jobs/986893/instock...   \n",
       "4  https://www.amazon.jobs/en/jobs/986891/shift-m...   \n",
       "\n",
       "                             title  \\\n",
       "0  Software Development Engineer I   \n",
       "1   Business Intelligence Engineer   \n",
       "2   Senior Solution Design Manager   \n",
       "3                  Instock Manager   \n",
       "4         Shift Manager, Logistics   \n",
       "\n",
       "                                         description           location  \\\n",
       "0  Amazon’s Global Logistics Technology team is c...    US, WA, Seattle   \n",
       "1  Amazon Advertising is dedicated to driving mea...  US, CA, Palo Alto   \n",
       "2  The Amazon Cross Border Exports team is lookin...   US, WA, Bellevue   \n",
       "3  The AmazonFresh and Prime Now organizations ar...    US, WA, Seattle   \n",
       "4  Do you want to work hard, have fun and make hi...    CA, AB, Calgary   \n",
       "\n",
       "                                 category  job_id  \\\n",
       "0                    Software Development  986897   \n",
       "1                   Business Intelligence  986896   \n",
       "2  Supply Chain/Transportation Management  986894   \n",
       "3  Buying, Planning, & Instock Management  986893   \n",
       "4     Fulfillment & Operations Management  986891   \n",
       "\n",
       "                     company_name  \n",
       "0       Amazon.com Services, Inc.  \n",
       "1       Amazon.com Services, Inc.  \n",
       "2       Amazon.com Services, Inc.  \n",
       "3       Amazon.com Services, Inc.  \n",
       "4  AMZN CAN Fulfillment Svcs, ULC  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have to examine the xml file to extract these column names from each xml nodes\n",
    "amazon_jobs_df_cols = [\"uniq_id\", \"crawl_timestamp\", \"job_url\", \"title\", \"description\", \"location\", \"category\", \\\n",
    "                       \"job_id\", \"company_name\"]\n",
    "amazon_jobs_rows = []\n",
    "\n",
    "# We interested in the Page node and all of it's elements\n",
    "for page in xroot.findall('page'):\n",
    "    for record in page.findall('record'):\n",
    "        s_uniq_id = record.find(\"uniq_id\").text\n",
    "        s_crawl_timestamp = record.find(\"crawl_timestamp\").text if record is not None else None\n",
    "        s_job_url = record.find(\"job_url\").text if record is not None else None\n",
    "        s_title = record.find(\"title\").text if record is not None else None\n",
    "        s_description = record.find(\"description\").text if record is not None else None\n",
    "        s_location = record.find(\"location\").text if record is not None else None\n",
    "        s_category = record.find(\"category\").text if record is not None else None\n",
    "        s_job_id = record.find(\"job_id\").text if record is not None else None\n",
    "        s_company_name = record.find(\"company_name\").text if record is not None else None\n",
    "\n",
    "        # Append the extract information into rows\n",
    "        amazon_jobs_rows.append({\"uniq_id\": s_uniq_id, \n",
    "                     \"crawl_timestamp\": s_crawl_timestamp, \n",
    "                     \"job_url\": s_job_url, \n",
    "                     \"title\": s_title,\n",
    "                     \"description\": s_description,\n",
    "                     \"location\": s_location,\n",
    "                     \"category\": s_category,\n",
    "                     \"job_id\": s_job_id,\n",
    "                     \"company_name\": s_company_name})\n",
    "\n",
    "# Convert list to Pandas dataframe\n",
    "amazon_jobs_pandas_df = pd.DataFrame(amazon_jobs_rows, columns = amazon_jobs_df_cols)\n",
    "amazon_jobs_pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon Jobs Posting Dataset Shape:(50, 9)\n"
     ]
    }
   ],
   "source": [
    "print(f'Amazon Jobs Posting Dataset Shape:{amazon_jobs_pandas_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. Identifies Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 OFLC H-1B Program Data (2011-2018)\n",
    "Let examine the data types for each columns and its summary statistics of the numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "H1B Dataframe Data Types:\n",
      "*****************************************\n",
      "fiscal_year             int64\n",
      "case_status            object\n",
      "case_submitted         object\n",
      "decision_date          object\n",
      "emp_name               object\n",
      "emp_city               object\n",
      "emp_state              object\n",
      "emp_zip               float64\n",
      "job_title              object\n",
      "soc_code               object\n",
      "soc_name               object\n",
      "naics_code             object\n",
      "full_time_position    float64\n",
      "prevailing_wage       float64\n",
      "pw_unit                object\n",
      "wage_from             float64\n",
      "wage_to               float64\n",
      "wage_unit              object\n",
      "work_city              object\n",
      "work_state             object\n",
      "work_zip              float64\n",
      "dtype: object\n",
      "\n",
      "\n",
      "*****************************************\n",
      "H1B Dataframe Summary Statistics:\n",
      "*****************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>emp_zip</th>\n",
       "      <th>full_time_position</th>\n",
       "      <th>prevailing_wage</th>\n",
       "      <th>wage_from</th>\n",
       "      <th>wage_to</th>\n",
       "      <th>work_zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.19209e+06</td>\n",
       "      <td>4.1911e+06</td>\n",
       "      <td>3.55783e+06</td>\n",
       "      <td>4.19149e+06</td>\n",
       "      <td>4.19167e+06</td>\n",
       "      <td>2.36199e+06</td>\n",
       "      <td>2.4886e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2014.94</td>\n",
       "      <td>47323.6</td>\n",
       "      <td>0.972199</td>\n",
       "      <td>69744.6</td>\n",
       "      <td>81438.9</td>\n",
       "      <td>86294.2</td>\n",
       "      <td>50989.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.20426</td>\n",
       "      <td>33546.8</td>\n",
       "      <td>0.164401</td>\n",
       "      <td>876329</td>\n",
       "      <td>4.24533e+06</td>\n",
       "      <td>7.15738e+07</td>\n",
       "      <td>33518.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2013</td>\n",
       "      <td>11747</td>\n",
       "      <td>1</td>\n",
       "      <td>53596</td>\n",
       "      <td>60000</td>\n",
       "      <td>0</td>\n",
       "      <td>19355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2015</td>\n",
       "      <td>45216</td>\n",
       "      <td>1</td>\n",
       "      <td>66726</td>\n",
       "      <td>72420</td>\n",
       "      <td>0</td>\n",
       "      <td>48374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2017</td>\n",
       "      <td>77046</td>\n",
       "      <td>1</td>\n",
       "      <td>84386</td>\n",
       "      <td>93557</td>\n",
       "      <td>87000</td>\n",
       "      <td>85034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2018</td>\n",
       "      <td>99988</td>\n",
       "      <td>1</td>\n",
       "      <td>1e+09</td>\n",
       "      <td>7.27887e+09</td>\n",
       "      <td>1.1e+11</td>\n",
       "      <td>99929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fiscal_year     emp_zip full_time_position prevailing_wage  \\\n",
       "count  4.19209e+06  4.1911e+06        3.55783e+06     4.19149e+06   \n",
       "mean       2014.94     47323.6           0.972199         69744.6   \n",
       "std        2.20426     33546.8           0.164401          876329   \n",
       "min           2011           1                  0               0   \n",
       "25%           2013       11747                  1           53596   \n",
       "50%           2015       45216                  1           66726   \n",
       "75%           2017       77046                  1           84386   \n",
       "max           2018       99988                  1           1e+09   \n",
       "\n",
       "         wage_from      wage_to    work_zip  \n",
       "count  4.19167e+06  2.36199e+06  2.4886e+06  \n",
       "mean       81438.9      86294.2     50989.7  \n",
       "std    4.24533e+06  7.15738e+07     33518.3  \n",
       "min              0            0           0  \n",
       "25%          60000            0       19355  \n",
       "50%          72420            0       48374  \n",
       "75%          93557        87000       85034  \n",
       "max    7.27887e+09      1.1e+11       99929  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('*****************************************')\n",
    "print('H1B Dataframe Data Types:')\n",
    "print('*****************************************')\n",
    "print(h1b_pandas_df.dtypes)\n",
    "print('\\n')\n",
    "print('*****************************************')\n",
    "print('H1B Dataframe Summary Statistics:')\n",
    "print('*****************************************')\n",
    "# Format each describable columns in General format\n",
    "h1b_pandas_df.describe().apply(lambda s: s.apply(lambda x: format(x, 'g')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above statistics, we can clearly see that some of the numeric columns have missing values (min value is equal to **zero**):\n",
    "1. **Employer Zip**\n",
    "2. **Prevailing Wage**\n",
    "3. **Wage From**\n",
    "4. **Wage To**\n",
    "5. **Work Zip**\n",
    "\n",
    "Next, we will examine if is there any missing values on the **Work City & State** columns on the **H1B** dataset.\n",
    "<br />\n",
    "**<font color=red>Note:</font>** These two columns will be our **<font color=blue>partitions keys</font>** of our fact tables. So, it's very important we must find these rows which have missing values and remove them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does H1B Dataset has any missing value on either work_city or work_state columns? True\n",
      "\n",
      "\n",
      "How many rows missing per each columns?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "work_city     538\n",
       "work_state    478\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Does H1B Dataset has any missing value on either work_city or work_state columns? {h1b_pandas_df.loc[:, [\"work_city\", \"work_state\"]].isnull().values.any()}')\n",
    "print('\\n')\n",
    "print('How many rows missing per each columns?')\n",
    "h1b_pandas_df.loc[:, [\"work_city\", \"work_state\"]].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "H1B Missing Values Dataframe:\n",
      "*****************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>case_status</th>\n",
       "      <th>case_submitted</th>\n",
       "      <th>decision_date</th>\n",
       "      <th>emp_name</th>\n",
       "      <th>emp_city</th>\n",
       "      <th>emp_state</th>\n",
       "      <th>emp_zip</th>\n",
       "      <th>job_title</th>\n",
       "      <th>soc_code</th>\n",
       "      <th>...</th>\n",
       "      <th>naics_code</th>\n",
       "      <th>full_time_position</th>\n",
       "      <th>prevailing_wage</th>\n",
       "      <th>pw_unit</th>\n",
       "      <th>wage_from</th>\n",
       "      <th>wage_to</th>\n",
       "      <th>wage_unit</th>\n",
       "      <th>work_city</th>\n",
       "      <th>work_state</th>\n",
       "      <th>work_zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2011</td>\n",
       "      <td>W</td>\n",
       "      <td>2/22/2011</td>\n",
       "      <td>2/22/2011</td>\n",
       "      <td>BIRLASOFT INC</td>\n",
       "      <td>EDISON</td>\n",
       "      <td>NJ</td>\n",
       "      <td>8817.0</td>\n",
       "      <td>SYSTEMS ANALYST</td>\n",
       "      <td>15-1051</td>\n",
       "      <td>...</td>\n",
       "      <td>541511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>2011</td>\n",
       "      <td>W</td>\n",
       "      <td>11/23/2010</td>\n",
       "      <td>11/23/2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>2011</td>\n",
       "      <td>W</td>\n",
       "      <td>2/10/2011</td>\n",
       "      <td>2/10/2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>2011</td>\n",
       "      <td>W</td>\n",
       "      <td>3/25/2011</td>\n",
       "      <td>3/25/2011</td>\n",
       "      <td>SKY FOUNDATION</td>\n",
       "      <td>OKLAHOMA CITY</td>\n",
       "      <td>OK</td>\n",
       "      <td>73106.0</td>\n",
       "      <td>BUSINESS MANAGER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>611110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>2011</td>\n",
       "      <td>W</td>\n",
       "      <td>3/25/2011</td>\n",
       "      <td>3/25/2011</td>\n",
       "      <td>RIVERWALK FOUNDATION</td>\n",
       "      <td>SAN ANTONIO</td>\n",
       "      <td>TX</td>\n",
       "      <td>78209.0</td>\n",
       "      <td>ASSISTANT PRINCIPAL</td>\n",
       "      <td>11-9032</td>\n",
       "      <td>...</td>\n",
       "      <td>611110</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     fiscal_year case_status case_submitted decision_date  \\\n",
       "41          2011           W      2/22/2011     2/22/2011   \n",
       "186         2011           W     11/23/2010    11/23/2010   \n",
       "292         2011           W      2/10/2011     2/10/2011   \n",
       "375         2011           W      3/25/2011     3/25/2011   \n",
       "398         2011           W      3/25/2011     3/25/2011   \n",
       "\n",
       "                 emp_name       emp_city emp_state  emp_zip  \\\n",
       "41          BIRLASOFT INC         EDISON        NJ   8817.0   \n",
       "186                   NaN            NaN       NaN      NaN   \n",
       "292                   NaN            NaN       NaN      NaN   \n",
       "375        SKY FOUNDATION  OKLAHOMA CITY        OK  73106.0   \n",
       "398  RIVERWALK FOUNDATION    SAN ANTONIO        TX  78209.0   \n",
       "\n",
       "               job_title soc_code  ... naics_code full_time_position  \\\n",
       "41       SYSTEMS ANALYST  15-1051  ...     541511                1.0   \n",
       "186                  NaN      NaN  ...        NaN                NaN   \n",
       "292                  NaN      NaN  ...        NaN                NaN   \n",
       "375     BUSINESS MANAGER      NaN  ...     611110                NaN   \n",
       "398  ASSISTANT PRINCIPAL  11-9032  ...     611110                1.0   \n",
       "\n",
       "     prevailing_wage  pw_unit wage_from  wage_to  wage_unit work_city  \\\n",
       "41               NaN        Y       NaN      NaN          Y       NaN   \n",
       "186              NaN      NaN       NaN      NaN        NaN       NaN   \n",
       "292              NaN      NaN       NaN      NaN        NaN       NaN   \n",
       "375              NaN      NaN       NaN      NaN        NaN       NaN   \n",
       "398              NaN      NaN       NaN      NaN        NaN       NaN   \n",
       "\n",
       "    work_state work_zip  \n",
       "41         NaN      NaN  \n",
       "186        NaN      NaN  \n",
       "292        NaN      NaN  \n",
       "375        NaN      NaN  \n",
       "398        NaN      NaN  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('*****************************************')\n",
    "print('H1B Missing Values Dataframe:')\n",
    "print('*****************************************')\n",
    "h1b_missing_city_or_state_df = h1b_pandas_df[(h1b_pandas_df['work_city'].isnull()) | (h1b_pandas_df['work_state'].isnull())] \n",
    "h1b_missing_city_or_state_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1B Missing Values Dataset Shape:(595, 21)\n"
     ]
    }
   ],
   "source": [
    "print(f'H1B Missing Values Dataset Shape:{h1b_missing_city_or_state_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Indeed Job Posting Dataset\n",
    "Let examine the data types for each columns and its summary statistics of each numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Indeed Jobs Posting Dataframe Data Types:\n",
      "*****************************************\n",
      "Job Title              object\n",
      "Job Description       float64\n",
      "Job Type              float64\n",
      "Categories            float64\n",
      "Location               object\n",
      "City                   object\n",
      "State                  object\n",
      "Country                object\n",
      "Zip Code               object\n",
      "Address               float64\n",
      "Salary From           float64\n",
      "Salary To             float64\n",
      "Salary Period         float64\n",
      "Apply Url              object\n",
      "Apply Email           float64\n",
      "Employees             float64\n",
      "Industry              float64\n",
      "Company Name           object\n",
      "Employer Email        float64\n",
      "Employer Website      float64\n",
      "Employer Phone        float64\n",
      "Employer Logo          object\n",
      "Companydescription     object\n",
      "Employer Location      object\n",
      "Employer City          object\n",
      "Employer State         object\n",
      "Employer Country       object\n",
      "Employer Zip Code      object\n",
      "Uniq Id                object\n",
      "Crawl Timestamp        object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "*************************************************\n",
      "Indeed Jobs Posting Dataframe Summary Statistics:\n",
      "*************************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Description</th>\n",
       "      <th>Job Type</th>\n",
       "      <th>Categories</th>\n",
       "      <th>Address</th>\n",
       "      <th>Salary From</th>\n",
       "      <th>Salary To</th>\n",
       "      <th>Salary Period</th>\n",
       "      <th>Apply Email</th>\n",
       "      <th>Employees</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Employer Email</th>\n",
       "      <th>Employer Website</th>\n",
       "      <th>Employer Phone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Job Description  Job Type  Categories  Address  Salary From  Salary To  \\\n",
       "count              0.0       0.0         0.0      0.0          0.0        0.0   \n",
       "mean               NaN       NaN         NaN      NaN          NaN        NaN   \n",
       "std                NaN       NaN         NaN      NaN          NaN        NaN   \n",
       "min                NaN       NaN         NaN      NaN          NaN        NaN   \n",
       "25%                NaN       NaN         NaN      NaN          NaN        NaN   \n",
       "50%                NaN       NaN         NaN      NaN          NaN        NaN   \n",
       "75%                NaN       NaN         NaN      NaN          NaN        NaN   \n",
       "max                NaN       NaN         NaN      NaN          NaN        NaN   \n",
       "\n",
       "       Salary Period  Apply Email  Employees  Industry  Employer Email  \\\n",
       "count            0.0          0.0        0.0       0.0             0.0   \n",
       "mean             NaN          NaN        NaN       NaN             NaN   \n",
       "std              NaN          NaN        NaN       NaN             NaN   \n",
       "min              NaN          NaN        NaN       NaN             NaN   \n",
       "25%              NaN          NaN        NaN       NaN             NaN   \n",
       "50%              NaN          NaN        NaN       NaN             NaN   \n",
       "75%              NaN          NaN        NaN       NaN             NaN   \n",
       "max              NaN          NaN        NaN       NaN             NaN   \n",
       "\n",
       "       Employer Website  Employer Phone  \n",
       "count               0.0             0.0  \n",
       "mean                NaN             NaN  \n",
       "std                 NaN             NaN  \n",
       "min                 NaN             NaN  \n",
       "25%                 NaN             NaN  \n",
       "50%                 NaN             NaN  \n",
       "75%                 NaN             NaN  \n",
       "max                 NaN             NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('*****************************************')\n",
    "print('Indeed Jobs Posting Dataframe Data Types:')\n",
    "print('*****************************************')\n",
    "print(indeed_jobs_pandas_df.dtypes)\n",
    "print('\\n')\n",
    "print('*************************************************')\n",
    "print('Indeed Jobs Posting Dataframe Summary Statistics:')\n",
    "print('*************************************************')\n",
    "# Format each describable columns in General format\n",
    "indeed_jobs_pandas_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above statistics, we can't see much of any useful information. However, we can say that all of these numeric columns have missing or malformed values (since the summary statistics return **NaN**). \n",
    "\n",
    "**Note:** We probably will <font color=red>drop some of these columns</font> since not all of them are not useful to our scenario. (For example, all information related to Employer since these information didn't help our audiences to make their decisions where they want to relocate after graduation. The more relevant information will be **Work City & State** since that's the place they will perform their work if they got hired.\n",
    "\n",
    "- **Job Type**\n",
    "- **Categories**\n",
    "- **Address**\n",
    "- **Salary From**\n",
    "- **Salary To**\n",
    "- **Salary Period**\n",
    "- **Apply Email**\n",
    "- **Employees**\n",
    "- **Industry**\n",
    "- **Employer Email**\n",
    "- **Employer Website**\n",
    "- **Employer Phone**\n",
    "\n",
    "Next, we will examine if is there any missing values on the Work City & State columns on the Indeed Jobs Posting dataset.<br /> \n",
    "**<font color=red>Note:</font>** These two columns can be joined with the **<font color=blue>partition keys</font>** of the fact table to create ad-hoc query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does Indeed Jobs Posting Dataset has any missing value on either Job City or State columns? False\n",
      "\n",
      "\n",
      "How many rows missing per each columns?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "City     0\n",
       "State    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Does Indeed Jobs Posting Dataset has any missing value on either Job City or State columns? {indeed_jobs_pandas_df.loc[:, [\"City\", \"State\"]].isnull().values.any()}')\n",
    "print('\\n')\n",
    "print('How many rows missing per each columns?')\n",
    "indeed_jobs_pandas_df.loc[:, [\"City\", \"State\"]].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nice!** The Indeed Jobs Posting Dataset doesn't have any missing rows on our focus columns (**Job City** and **Job State**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3 Job Posting On Amazon Jobs\n",
    "Let examine the data types for each columns and its summary statistics of the numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Amazon Jobs Posting Dataframe Data Types:\n",
      "*****************************************\n",
      "uniq_id            object\n",
      "crawl_timestamp    object\n",
      "job_url            object\n",
      "title              object\n",
      "description        object\n",
      "location           object\n",
      "category           object\n",
      "job_id             object\n",
      "company_name       object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "*************************************************\n",
      "Amazon Jobs Posting Dataframe Summary Statistics:\n",
      "*************************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniq_id</th>\n",
       "      <th>crawl_timestamp</th>\n",
       "      <th>job_url</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>category</th>\n",
       "      <th>job_id</th>\n",
       "      <th>company_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>45</td>\n",
       "      <td>47</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>0c96a59282dc24bbc546bf31d42825d5</td>\n",
       "      <td>2019-11-15 00:16:13 +0000</td>\n",
       "      <td>https://www.amazon.jobs/en/jobs/977743/sr-tech...</td>\n",
       "      <td>Software Development Engineer</td>\n",
       "      <td>Interested in working on IoT products, AWS clo...</td>\n",
       "      <td>US, WA, Seattle</td>\n",
       "      <td>Software Development</td>\n",
       "      <td>977767</td>\n",
       "      <td>Amazon.com Services, Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 uniq_id            crawl_timestamp  \\\n",
       "count                                 50                         50   \n",
       "unique                                50                          5   \n",
       "top     0c96a59282dc24bbc546bf31d42825d5  2019-11-15 00:16:13 +0000   \n",
       "freq                                   1                         10   \n",
       "\n",
       "                                                  job_url  \\\n",
       "count                                                  50   \n",
       "unique                                                 50   \n",
       "top     https://www.amazon.jobs/en/jobs/977743/sr-tech...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                title  \\\n",
       "count                              50   \n",
       "unique                             45   \n",
       "top     Software Development Engineer   \n",
       "freq                                4   \n",
       "\n",
       "                                              description         location  \\\n",
       "count                                                  50               50   \n",
       "unique                                                 47               25   \n",
       "top     Interested in working on IoT products, AWS clo...  US, WA, Seattle   \n",
       "freq                                                    3               15   \n",
       "\n",
       "                    category  job_id               company_name  \n",
       "count                     50      50                         50  \n",
       "unique                    20      50                         18  \n",
       "top     Software Development  977767  Amazon.com Services, Inc.  \n",
       "freq                       9       1                         23  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('*****************************************')\n",
    "print('Amazon Jobs Posting Dataframe Data Types:')\n",
    "print('*****************************************')\n",
    "print(amazon_jobs_pandas_df.dtypes)\n",
    "print('\\n')\n",
    "print('*************************************************')\n",
    "print('Amazon Jobs Posting Dataframe Summary Statistics:')\n",
    "print('*************************************************')\n",
    "# Describe this dataframe\n",
    "amazon_jobs_pandas_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't tell if there are any missing value based on the above statistics. However, the more serious problem is that this datafram doesn't have **City** and **State** cloumns. We need to extract these information from **location** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniq_id</th>\n",
       "      <th>crawl_timestamp</th>\n",
       "      <th>job_url</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>category</th>\n",
       "      <th>job_id</th>\n",
       "      <th>company_name</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>850f0199e2ce9d1d51ee5e2e11318d94</td>\n",
       "      <td>2019-11-15 00:15:23 +0000</td>\n",
       "      <td>https://www.amazon.jobs/en/jobs/986897/softwar...</td>\n",
       "      <td>Software Development Engineer I</td>\n",
       "      <td>Amazon’s Global Logistics Technology team is c...</td>\n",
       "      <td>US, WA, Seattle</td>\n",
       "      <td>Software Development</td>\n",
       "      <td>986897</td>\n",
       "      <td>Amazon.com Services, Inc.</td>\n",
       "      <td>US</td>\n",
       "      <td>WA</td>\n",
       "      <td>Seattle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9215700a26684c9dca1ad77718f33ff2</td>\n",
       "      <td>2019-11-15 00:15:23 +0000</td>\n",
       "      <td>https://www.amazon.jobs/en/jobs/986896/busines...</td>\n",
       "      <td>Business Intelligence Engineer</td>\n",
       "      <td>Amazon Advertising is dedicated to driving mea...</td>\n",
       "      <td>US, CA, Palo Alto</td>\n",
       "      <td>Business Intelligence</td>\n",
       "      <td>986896</td>\n",
       "      <td>Amazon.com Services, Inc.</td>\n",
       "      <td>US</td>\n",
       "      <td>CA</td>\n",
       "      <td>Palo Alto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14b00d40ced925890eb4eca458040133</td>\n",
       "      <td>2019-11-15 00:15:23 +0000</td>\n",
       "      <td>https://www.amazon.jobs/en/jobs/986894/senior-...</td>\n",
       "      <td>Senior Solution Design Manager</td>\n",
       "      <td>The Amazon Cross Border Exports team is lookin...</td>\n",
       "      <td>US, WA, Bellevue</td>\n",
       "      <td>Supply Chain/Transportation Management</td>\n",
       "      <td>986894</td>\n",
       "      <td>Amazon.com Services, Inc.</td>\n",
       "      <td>US</td>\n",
       "      <td>WA</td>\n",
       "      <td>Bellevue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0c96a59282dc24bbc546bf31d42825d5</td>\n",
       "      <td>2019-11-15 00:15:23 +0000</td>\n",
       "      <td>https://www.amazon.jobs/en/jobs/986893/instock...</td>\n",
       "      <td>Instock Manager</td>\n",
       "      <td>The AmazonFresh and Prime Now organizations ar...</td>\n",
       "      <td>US, WA, Seattle</td>\n",
       "      <td>Buying, Planning, &amp; Instock Management</td>\n",
       "      <td>986893</td>\n",
       "      <td>Amazon.com Services, Inc.</td>\n",
       "      <td>US</td>\n",
       "      <td>WA</td>\n",
       "      <td>Seattle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fff278b8672d6999631c5517c081fc4d</td>\n",
       "      <td>2019-11-15 00:15:23 +0000</td>\n",
       "      <td>https://www.amazon.jobs/en/jobs/986891/shift-m...</td>\n",
       "      <td>Shift Manager, Logistics</td>\n",
       "      <td>Do you want to work hard, have fun and make hi...</td>\n",
       "      <td>CA, AB, Calgary</td>\n",
       "      <td>Fulfillment &amp; Operations Management</td>\n",
       "      <td>986891</td>\n",
       "      <td>AMZN CAN Fulfillment Svcs, ULC</td>\n",
       "      <td>CA</td>\n",
       "      <td>AB</td>\n",
       "      <td>Calgary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            uniq_id            crawl_timestamp  \\\n",
       "0  850f0199e2ce9d1d51ee5e2e11318d94  2019-11-15 00:15:23 +0000   \n",
       "1  9215700a26684c9dca1ad77718f33ff2  2019-11-15 00:15:23 +0000   \n",
       "2  14b00d40ced925890eb4eca458040133  2019-11-15 00:15:23 +0000   \n",
       "3  0c96a59282dc24bbc546bf31d42825d5  2019-11-15 00:15:23 +0000   \n",
       "4  fff278b8672d6999631c5517c081fc4d  2019-11-15 00:15:23 +0000   \n",
       "\n",
       "                                             job_url  \\\n",
       "0  https://www.amazon.jobs/en/jobs/986897/softwar...   \n",
       "1  https://www.amazon.jobs/en/jobs/986896/busines...   \n",
       "2  https://www.amazon.jobs/en/jobs/986894/senior-...   \n",
       "3  https://www.amazon.jobs/en/jobs/986893/instock...   \n",
       "4  https://www.amazon.jobs/en/jobs/986891/shift-m...   \n",
       "\n",
       "                             title  \\\n",
       "0  Software Development Engineer I   \n",
       "1   Business Intelligence Engineer   \n",
       "2   Senior Solution Design Manager   \n",
       "3                  Instock Manager   \n",
       "4         Shift Manager, Logistics   \n",
       "\n",
       "                                         description           location  \\\n",
       "0  Amazon’s Global Logistics Technology team is c...    US, WA, Seattle   \n",
       "1  Amazon Advertising is dedicated to driving mea...  US, CA, Palo Alto   \n",
       "2  The Amazon Cross Border Exports team is lookin...   US, WA, Bellevue   \n",
       "3  The AmazonFresh and Prime Now organizations ar...    US, WA, Seattle   \n",
       "4  Do you want to work hard, have fun and make hi...    CA, AB, Calgary   \n",
       "\n",
       "                                 category  job_id  \\\n",
       "0                    Software Development  986897   \n",
       "1                   Business Intelligence  986896   \n",
       "2  Supply Chain/Transportation Management  986894   \n",
       "3  Buying, Planning, & Instock Management  986893   \n",
       "4     Fulfillment & Operations Management  986891   \n",
       "\n",
       "                     company_name Country State        City  \n",
       "0       Amazon.com Services, Inc.      US    WA     Seattle  \n",
       "1       Amazon.com Services, Inc.      US    CA   Palo Alto  \n",
       "2       Amazon.com Services, Inc.      US    WA    Bellevue  \n",
       "3       Amazon.com Services, Inc.      US    WA     Seattle  \n",
       "4  AMZN CAN Fulfillment Svcs, ULC      CA    AB     Calgary  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since this dataframe has only 50 rows, so it's ok to loop each rows\n",
    "for index, row in amazon_jobs_pandas_df.iterrows():\n",
    "    # Extract Country, City, State from location column\n",
    "    addresses = row['location'].split(',')\n",
    "    if len(addresses) == 3:\n",
    "        # This address has complete Country, State and City\n",
    "        amazon_jobs_pandas_df.loc[index,['Country']] = addresses[0]\n",
    "        amazon_jobs_pandas_df.loc[index,['State']] = addresses[1]\n",
    "        amazon_jobs_pandas_df.loc[index,['City']] = addresses[2]\n",
    "    elif len(addresses) == 2:\n",
    "        # This address has only Country and City (probably not in the U.S.)\n",
    "        amazon_jobs_pandas_df.loc[index,['Country']] = addresses[0]\n",
    "        amazon_jobs_pandas_df.loc[index,['State']] = None\n",
    "        amazon_jobs_pandas_df.loc[index,['City']] = addresses[1]\n",
    "    elif len(addresses) == 1:\n",
    "        # This address has only Country\n",
    "        amazon_jobs_pandas_df.loc[index,['Country']] = addresses[0]\n",
    "        amazon_jobs_pandas_df.loc[index,['State']] = None\n",
    "        amazon_jobs_pandas_df.loc[index,['City']] = None\n",
    "        \n",
    "# Check to see if new columns were created\n",
    "amazon_jobs_pandas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will examine if is there any missing values on the **Job City & State** columns on the **Amazon Jobs Posting** dataset.<br /> \n",
    "**<font color=red>Note:</font>** These two columns can be joined with the **<font color=blue>partition keys</font>** of the fact table to create ad-hoc query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does Amazon Jobs Posting Dataset has any missing value on either City or State columns? True\n",
      "\n",
      "\n",
      "How many rows missing per each columns?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "City     1\n",
       "State    4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Does Amazon Jobs Posting Dataset has any missing value on either City or State columns? {amazon_jobs_pandas_df.loc[:, [\"City\", \"State\"]].isnull().values.any()}')\n",
    "print('\\n')\n",
    "print('How many rows missing per each columns?')\n",
    "amazon_jobs_pandas_df.loc[:, [\"City\", \"State\"]].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************\n",
      "Amazon Jobs Posting Missing Values Dataframe:\n",
      "*********************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniq_id</th>\n",
       "      <th>crawl_timestamp</th>\n",
       "      <th>job_url</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>category</th>\n",
       "      <th>job_id</th>\n",
       "      <th>company_name</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5c376060d06977df7825cc49141c308b</td>\n",
       "      <td>2019-11-15 00:13:36 +0000</td>\n",
       "      <td>https://www.amazon.jobs/en/jobs/956568/sr-busi...</td>\n",
       "      <td>Sr. Business Development Manager – AWS High Pe...</td>\n",
       "      <td>Amazon Web Services (AWS) is the pioneer and r...</td>\n",
       "      <td>UK, London</td>\n",
       "      <td>Business &amp; Merchant Development</td>\n",
       "      <td>956568</td>\n",
       "      <td>AWS EMEA SARL (UK Branch)</td>\n",
       "      <td>UK</td>\n",
       "      <td>None</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6b745f0dfdfb8aba7d584d12bc712643</td>\n",
       "      <td>2019-11-15 00:13:07 +0000</td>\n",
       "      <td>https://www.amazon.jobs/en/jobs/960293/softwar...</td>\n",
       "      <td>Software Development Engineer</td>\n",
       "      <td>Prime Video is changing the way millions of cu...</td>\n",
       "      <td>UK, London</td>\n",
       "      <td>Software Development</td>\n",
       "      <td>960293</td>\n",
       "      <td>Amazon Dev Centre (London) Ltd</td>\n",
       "      <td>UK</td>\n",
       "      <td>None</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>82084f962c940000e13168f4d5a03657</td>\n",
       "      <td>2019-11-15 00:16:13 +0000</td>\n",
       "      <td>https://www.amazon.jobs/en/jobs/977623/dceo-cl...</td>\n",
       "      <td>DCEO Cluster Manager, Singapore</td>\n",
       "      <td>DCEO Manager SingaporeResponsible for the mana...</td>\n",
       "      <td>SG, Singapore</td>\n",
       "      <td>Operations, IT, &amp; Support Engineering</td>\n",
       "      <td>977623</td>\n",
       "      <td>Amzn Asia-PacificResources SGP</td>\n",
       "      <td>SG</td>\n",
       "      <td>None</td>\n",
       "      <td>Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>19c516821e775cfa56a6d785b4e703a4</td>\n",
       "      <td>2019-11-15 00:17:14 +0000</td>\n",
       "      <td>https://www.amazon.jobs/en/jobs/979735/senior-...</td>\n",
       "      <td>Senior Security Engineer - Hardware</td>\n",
       "      <td>Help us protect not only the Amazon Web Servic...</td>\n",
       "      <td>US</td>\n",
       "      <td>Systems, Quality, &amp; Security Engineering</td>\n",
       "      <td>979735</td>\n",
       "      <td>Amazon.com Services, Inc.</td>\n",
       "      <td>US</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             uniq_id            crawl_timestamp  \\\n",
       "12  5c376060d06977df7825cc49141c308b  2019-11-15 00:13:36 +0000   \n",
       "20  6b745f0dfdfb8aba7d584d12bc712643  2019-11-15 00:13:07 +0000   \n",
       "36  82084f962c940000e13168f4d5a03657  2019-11-15 00:16:13 +0000   \n",
       "43  19c516821e775cfa56a6d785b4e703a4  2019-11-15 00:17:14 +0000   \n",
       "\n",
       "                                              job_url  \\\n",
       "12  https://www.amazon.jobs/en/jobs/956568/sr-busi...   \n",
       "20  https://www.amazon.jobs/en/jobs/960293/softwar...   \n",
       "36  https://www.amazon.jobs/en/jobs/977623/dceo-cl...   \n",
       "43  https://www.amazon.jobs/en/jobs/979735/senior-...   \n",
       "\n",
       "                                                title  \\\n",
       "12  Sr. Business Development Manager – AWS High Pe...   \n",
       "20                      Software Development Engineer   \n",
       "36                    DCEO Cluster Manager, Singapore   \n",
       "43                Senior Security Engineer - Hardware   \n",
       "\n",
       "                                          description       location  \\\n",
       "12  Amazon Web Services (AWS) is the pioneer and r...     UK, London   \n",
       "20  Prime Video is changing the way millions of cu...     UK, London   \n",
       "36  DCEO Manager SingaporeResponsible for the mana...  SG, Singapore   \n",
       "43  Help us protect not only the Amazon Web Servic...             US   \n",
       "\n",
       "                                    category  job_id  \\\n",
       "12           Business & Merchant Development  956568   \n",
       "20                      Software Development  960293   \n",
       "36     Operations, IT, & Support Engineering  977623   \n",
       "43  Systems, Quality, & Security Engineering  979735   \n",
       "\n",
       "                      company_name Country State        City  \n",
       "12       AWS EMEA SARL (UK Branch)      UK  None      London  \n",
       "20  Amazon Dev Centre (London) Ltd      UK  None      London  \n",
       "36  Amzn Asia-PacificResources SGP      SG  None   Singapore  \n",
       "43       Amazon.com Services, Inc.      US  None        None  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('*********************************************')\n",
    "print('Amazon Jobs Posting Missing Values Dataframe:')\n",
    "print('*********************************************')\n",
    "amazon_jobs_missing_city_or_state_df = amazon_jobs_pandas_df[(amazon_jobs_pandas_df['City'].isnull()) | (amazon_jobs_pandas_df['State'].isnull())] \n",
    "amazon_jobs_missing_city_or_state_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3. Identifies Duplicate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 OFLC H-1B Program Data (2011-2018)\n",
    "Let examine if there any **duplicate** rows based on all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************\n",
      "Is there any duplicat rows on H1B Dataframe:\n",
      "********************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('********************************************')\n",
    "print('Is there any duplicat rows on H1B Dataframe:')\n",
    "print('********************************************')\n",
    "h1b_pandas_df.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the duplicate rows\n",
    "# keep = 'first' considers first value as unique and rest of the same values as duplicate\n",
    "h1b_duplicate_df = h1b_pandas_df[h1b_pandas_df.duplicated(keep = 'first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Rows except first occurrence based on all columns are :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>case_status</th>\n",
       "      <th>case_submitted</th>\n",
       "      <th>decision_date</th>\n",
       "      <th>emp_name</th>\n",
       "      <th>emp_city</th>\n",
       "      <th>emp_state</th>\n",
       "      <th>emp_zip</th>\n",
       "      <th>job_title</th>\n",
       "      <th>soc_code</th>\n",
       "      <th>...</th>\n",
       "      <th>naics_code</th>\n",
       "      <th>full_time_position</th>\n",
       "      <th>prevailing_wage</th>\n",
       "      <th>pw_unit</th>\n",
       "      <th>wage_from</th>\n",
       "      <th>wage_to</th>\n",
       "      <th>wage_unit</th>\n",
       "      <th>work_city</th>\n",
       "      <th>work_state</th>\n",
       "      <th>work_zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>2011</td>\n",
       "      <td>C</td>\n",
       "      <td>5/2/2011</td>\n",
       "      <td>5/6/2011</td>\n",
       "      <td>ALERIS ROLLED PRODUCTS, INC.</td>\n",
       "      <td>BEACHWOOD</td>\n",
       "      <td>OH</td>\n",
       "      <td>44122.0</td>\n",
       "      <td>SENIOR INDUSTRIAL ENGINEER</td>\n",
       "      <td>17-2112</td>\n",
       "      <td>...</td>\n",
       "      <td>331316</td>\n",
       "      <td>1.0</td>\n",
       "      <td>74298.00</td>\n",
       "      <td>Y</td>\n",
       "      <td>74298.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>LEWISPORT</td>\n",
       "      <td>KY</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>2011</td>\n",
       "      <td>C</td>\n",
       "      <td>7/28/2011</td>\n",
       "      <td>8/3/2011</td>\n",
       "      <td>WS ATKINS, INC.</td>\n",
       "      <td>HOUSTON</td>\n",
       "      <td>TX</td>\n",
       "      <td>77024.0</td>\n",
       "      <td>PRINCIPAL CONSULTANT</td>\n",
       "      <td>19-3032</td>\n",
       "      <td>...</td>\n",
       "      <td>541990</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99445.00</td>\n",
       "      <td>Y</td>\n",
       "      <td>165000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>HOUSTON</td>\n",
       "      <td>TX</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>2011</td>\n",
       "      <td>C</td>\n",
       "      <td>8/30/2011</td>\n",
       "      <td>9/6/2011</td>\n",
       "      <td>CRISPIN PORTER &amp; BOGUSKY, LLC</td>\n",
       "      <td>MIAMI</td>\n",
       "      <td>FL</td>\n",
       "      <td>33133.0</td>\n",
       "      <td>COPYWRITER</td>\n",
       "      <td>27-3043</td>\n",
       "      <td>...</td>\n",
       "      <td>541810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32178.00</td>\n",
       "      <td>Y</td>\n",
       "      <td>43000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>BOULDER</td>\n",
       "      <td>CO</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>2011</td>\n",
       "      <td>C</td>\n",
       "      <td>8/30/2011</td>\n",
       "      <td>9/6/2011</td>\n",
       "      <td>CRISPIN PORTER &amp; BOGUSKY, LLC</td>\n",
       "      <td>MIAMI</td>\n",
       "      <td>FL</td>\n",
       "      <td>33133.0</td>\n",
       "      <td>ART DIRECTOR</td>\n",
       "      <td>27-1011</td>\n",
       "      <td>...</td>\n",
       "      <td>541810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43300.00</td>\n",
       "      <td>Y</td>\n",
       "      <td>43300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>BOULDER</td>\n",
       "      <td>CO</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>2011</td>\n",
       "      <td>C</td>\n",
       "      <td>11/8/2010</td>\n",
       "      <td>11/15/2010</td>\n",
       "      <td>MUSE MANAGEMENT, INC.</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>NY</td>\n",
       "      <td>10010.0</td>\n",
       "      <td>PROFESSIONAL FASHION MODEL</td>\n",
       "      <td>41-9012</td>\n",
       "      <td>...</td>\n",
       "      <td>711410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.87</td>\n",
       "      <td>H</td>\n",
       "      <td>100.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>H</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>NY</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4192041</th>\n",
       "      <td>2018</td>\n",
       "      <td>C</td>\n",
       "      <td>2/8/18</td>\n",
       "      <td>2/15/18</td>\n",
       "      <td>DELOITTE CONSULTING LLP</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>PA</td>\n",
       "      <td>19103.0</td>\n",
       "      <td>CONSULTANT</td>\n",
       "      <td>15-1121</td>\n",
       "      <td>...</td>\n",
       "      <td>54161</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57096.00</td>\n",
       "      <td>Y</td>\n",
       "      <td>57096.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>PHILADELPHIA</td>\n",
       "      <td>PA</td>\n",
       "      <td>19103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4192042</th>\n",
       "      <td>2018</td>\n",
       "      <td>C</td>\n",
       "      <td>2/21/18</td>\n",
       "      <td>2/27/18</td>\n",
       "      <td>TECH MAHINDRA (AMERICAS),INC.</td>\n",
       "      <td>SOUTH PLAINFIELD</td>\n",
       "      <td>NJ</td>\n",
       "      <td>7080.0</td>\n",
       "      <td>NETWORK ARCHITECT</td>\n",
       "      <td>15-1143</td>\n",
       "      <td>...</td>\n",
       "      <td>541511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99549.00</td>\n",
       "      <td>Y</td>\n",
       "      <td>99549.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>BOXBOROUGH</td>\n",
       "      <td>MA</td>\n",
       "      <td>1719.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4192050</th>\n",
       "      <td>2018</td>\n",
       "      <td>C</td>\n",
       "      <td>3/22/18</td>\n",
       "      <td>3/28/18</td>\n",
       "      <td>NAMITUS TECHNOLOGIES, INC.</td>\n",
       "      <td>FRISCO</td>\n",
       "      <td>TX</td>\n",
       "      <td>75035.0</td>\n",
       "      <td>SOFTWARE DEVELOPER</td>\n",
       "      <td>15-1132</td>\n",
       "      <td>...</td>\n",
       "      <td>541511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90646.00</td>\n",
       "      <td>Y</td>\n",
       "      <td>90646.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>FRISCO</td>\n",
       "      <td>TX</td>\n",
       "      <td>75035.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4192052</th>\n",
       "      <td>2018</td>\n",
       "      <td>C</td>\n",
       "      <td>3/16/18</td>\n",
       "      <td>3/22/18</td>\n",
       "      <td>ADBAKX LLC</td>\n",
       "      <td>MONMOUTH JUNCTION</td>\n",
       "      <td>NJ</td>\n",
       "      <td>8852.0</td>\n",
       "      <td>BUSINESS ANALYST</td>\n",
       "      <td>15-1121</td>\n",
       "      <td>...</td>\n",
       "      <td>541511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>74734.00</td>\n",
       "      <td>Y</td>\n",
       "      <td>74734.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>ATLANTA</td>\n",
       "      <td>GA</td>\n",
       "      <td>30308.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4192055</th>\n",
       "      <td>2018</td>\n",
       "      <td>C</td>\n",
       "      <td>3/11/18</td>\n",
       "      <td>3/15/18</td>\n",
       "      <td>TATA CONSULTANCY SERVICES LIMITED</td>\n",
       "      <td>ROCKVILLE</td>\n",
       "      <td>MD</td>\n",
       "      <td>20850.0</td>\n",
       "      <td>DATABASE ADMINISTRATOR</td>\n",
       "      <td>15-1141</td>\n",
       "      <td>...</td>\n",
       "      <td>541511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>74339.00</td>\n",
       "      <td>Y</td>\n",
       "      <td>74339.0</td>\n",
       "      <td>91100.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>SUNNYVALE</td>\n",
       "      <td>CA</td>\n",
       "      <td>94085.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>353129 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         fiscal_year case_status case_submitted decision_date  \\\n",
       "224             2011           C       5/2/2011      5/6/2011   \n",
       "232             2011           C      7/28/2011      8/3/2011   \n",
       "234             2011           C      8/30/2011      9/6/2011   \n",
       "236             2011           C      8/30/2011      9/6/2011   \n",
       "277             2011           C      11/8/2010    11/15/2010   \n",
       "...              ...         ...            ...           ...   \n",
       "4192041         2018           C         2/8/18       2/15/18   \n",
       "4192042         2018           C        2/21/18       2/27/18   \n",
       "4192050         2018           C        3/22/18       3/28/18   \n",
       "4192052         2018           C        3/16/18       3/22/18   \n",
       "4192055         2018           C        3/11/18       3/15/18   \n",
       "\n",
       "                                  emp_name           emp_city emp_state  \\\n",
       "224           ALERIS ROLLED PRODUCTS, INC.          BEACHWOOD        OH   \n",
       "232                        WS ATKINS, INC.            HOUSTON        TX   \n",
       "234          CRISPIN PORTER & BOGUSKY, LLC              MIAMI        FL   \n",
       "236          CRISPIN PORTER & BOGUSKY, LLC              MIAMI        FL   \n",
       "277                  MUSE MANAGEMENT, INC.           NEW YORK        NY   \n",
       "...                                    ...                ...       ...   \n",
       "4192041            DELOITTE CONSULTING LLP       PHILADELPHIA        PA   \n",
       "4192042      TECH MAHINDRA (AMERICAS),INC.   SOUTH PLAINFIELD        NJ   \n",
       "4192050         NAMITUS TECHNOLOGIES, INC.             FRISCO        TX   \n",
       "4192052                         ADBAKX LLC  MONMOUTH JUNCTION        NJ   \n",
       "4192055  TATA CONSULTANCY SERVICES LIMITED          ROCKVILLE        MD   \n",
       "\n",
       "         emp_zip                   job_title soc_code  ... naics_code  \\\n",
       "224      44122.0  SENIOR INDUSTRIAL ENGINEER  17-2112  ...     331316   \n",
       "232      77024.0        PRINCIPAL CONSULTANT  19-3032  ...     541990   \n",
       "234      33133.0                  COPYWRITER  27-3043  ...     541810   \n",
       "236      33133.0                ART DIRECTOR  27-1011  ...     541810   \n",
       "277      10010.0  PROFESSIONAL FASHION MODEL  41-9012  ...     711410   \n",
       "...          ...                         ...      ...  ...        ...   \n",
       "4192041  19103.0                  CONSULTANT  15-1121  ...      54161   \n",
       "4192042   7080.0           NETWORK ARCHITECT  15-1143  ...     541511   \n",
       "4192050  75035.0          SOFTWARE DEVELOPER  15-1132  ...     541511   \n",
       "4192052   8852.0            BUSINESS ANALYST  15-1121  ...     541511   \n",
       "4192055  20850.0      DATABASE ADMINISTRATOR  15-1141  ...     541511   \n",
       "\n",
       "        full_time_position  prevailing_wage  pw_unit wage_from  wage_to  \\\n",
       "224                    1.0         74298.00        Y   74298.0      NaN   \n",
       "232                    1.0         99445.00        Y  165000.0      NaN   \n",
       "234                    1.0         32178.00        Y   43000.0      NaN   \n",
       "236                    1.0         43300.00        Y   43300.0      NaN   \n",
       "277                    0.0            21.87        H     100.0    500.0   \n",
       "...                    ...              ...      ...       ...      ...   \n",
       "4192041                1.0         57096.00        Y   57096.0      0.0   \n",
       "4192042                1.0         99549.00        Y   99549.0      0.0   \n",
       "4192050                1.0         90646.00        Y   90646.0      0.0   \n",
       "4192052                1.0         74734.00        Y   74734.0      0.0   \n",
       "4192055                1.0         74339.00        Y   74339.0  91100.0   \n",
       "\n",
       "         wage_unit     work_city work_state work_zip  \n",
       "224              Y     LEWISPORT         KY      NaN  \n",
       "232              Y       HOUSTON         TX      NaN  \n",
       "234              Y       BOULDER         CO      NaN  \n",
       "236              Y       BOULDER         CO      NaN  \n",
       "277              H      NEW YORK         NY      NaN  \n",
       "...            ...           ...        ...      ...  \n",
       "4192041          Y  PHILADELPHIA         PA  19103.0  \n",
       "4192042          Y    BOXBOROUGH         MA   1719.0  \n",
       "4192050          Y        FRISCO         TX  75035.0  \n",
       "4192052          Y       ATLANTA         GA  30308.0  \n",
       "4192055          Y     SUNNYVALE         CA  94085.0  \n",
       "\n",
       "[353129 rows x 21 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Duplicate Rows except first occurrence based on all columns are :\")\n",
    "h1b_duplicate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1B Duplicate Rows Dataset Shape:(353129, 21)\n"
     ]
    }
   ],
   "source": [
    "print(f'H1B Duplicate Rows Dataset Shape:{h1b_duplicate_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Indeed Job Posting Dataset\n",
    "Let examine if there any duplicate rows based on all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "Is there any duplicat rows on Indeed Jobs Posting Dataframe:\n",
      "************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('************************************************************')\n",
    "print('Is there any duplicat rows on Indeed Jobs Posting Dataframe:')\n",
    "print('************************************************************')\n",
    "indeed_jobs_pandas_df.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nice!** The Indeed Jobs Posting Dataset doesn't have any duplicate rows consodered all columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3 Job Posting On Amazon Jobs\n",
    "Let examine if there any duplicate rows based on all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "Is there any duplicat rows on Amazon Jobs Posting Dataframe:\n",
      "***********************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('************************************************************')\n",
    "print('Is there any duplicat rows on Amazon Jobs Posting Dataframe:')\n",
    "print('***********************************************************')\n",
    "amazon_jobs_pandas_df.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nice!** The Amazon Jobs Posting Dataset doesn't have any duplicate rows consodered all columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Cleaning Steps\n",
    "Steps necessary to clean the data are:\n",
    "1. Remove missing value's rows (if any). In our case, we will consider if the row has missing value if either or both **City & State** are missing.\n",
    "2. Remove the duplicate rows (if any). We consider if the row is duplicate when data from all columns are duplicated. In our case, we want to keep the first occurance and disregard the rest of the same value's rows.\n",
    "\n",
    "**<center>Below is the summary of what we found on each datasets</center>**\n",
    "\n",
    "| Dataset | Has Missing Values | Has Duplicate Rows |\n",
    "|------|------|------|\n",
    "|   H1B  | Yes | Yes |\n",
    "|   Indeed Jobs Posting  | No | No |\n",
    "|   Amazon Jobs Posting  | Yes | No |\n",
    "\n",
    "Therefore, we will perform **Data Cleansing** on the two datasets:\n",
    "1. **OFLC H-1B Program Data (2011-2018)**\n",
    "2. **Job Posting On Amazon Jobs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 OFLC H-1B Program Data (2011-2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing value's row \n",
    "# Select the rows that have both work city and state values\n",
    "cleaned_h1b_df = h1b_pandas_df[~(h1b_pandas_df['work_city'].isnull() | h1b_pandas_df['work_state'].isnull())]\n",
    "\n",
    "# Drop duplicate rows\n",
    "cleaned_h1b_df = cleaned_h1b_df[~cleaned_h1b_df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does Cleaned H1B Dataset has any missing value on either City or State columns? False\n",
      "\n",
      "\n",
      "Does Cleaned H1B Dataset has any duplicate rows? False\n"
     ]
    }
   ],
   "source": [
    "print(f'Does Cleaned H1B Dataset has any missing value on either City or State columns? {cleaned_h1b_df.loc[:, [\"work_city\", \"work_state\"]].isnull().values.any()}')\n",
    "print('\\n')\n",
    "print(f'Does Cleaned H1B Dataset has any duplicate rows? {cleaned_h1b_df.duplicated().any()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean H1B Dataset Shape:(3838578, 21)\n"
     ]
    }
   ],
   "source": [
    "print(f'Clean H1B Dataset Shape:{cleaned_h1b_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Job Posting On Amazon Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing value's row \n",
    "# Select the rows that have both work city and state values\n",
    "cleaned_amazon_jobs_df = amazon_jobs_pandas_df[~(amazon_jobs_pandas_df['City'].isnull() \\\n",
    "                                               | amazon_jobs_pandas_df['State'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does Cleaned Amazon Jobs Posting Dataset has any missing value on either City or State columns? False\n"
     ]
    }
   ],
   "source": [
    "print(f'Does Cleaned Amazon Jobs Posting Dataset has any missing value on either City or State columns? {cleaned_amazon_jobs_df.loc[:, [\"City\", \"State\"]].isnull().values.any()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Amazon Jobs Posting Dataset Shape:(46, 12)\n"
     ]
    }
   ],
   "source": [
    "print(f'Clean Amazon Jobs Posting Dataset Shape:{cleaned_amazon_jobs_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Below is a simple conceptual data model of this project.\n",
    "<img src=\"Conceptual Data Model.png\"/>\n",
    "\n",
    "**<center>Database (Star) Schema</center>**\n",
    " \n",
    "There are two main aspects I'd like to address:\n",
    "1. The reason why I choose to design tables in a **star schema** is due to its simplicity. Star schema join-logic is generally simple to write a logic and also easy for fast aggregations.  \n",
    "2. The reason why I select **Work City & State** as the partition keys for fact table's parquet files is that:\n",
    "\n",
    "   `Based on my personel experience, I always put the Where as a filter everytime when I search for a job since I really don't want to relocate. I think the choice of how to choose the partition keys is really depends and there is no absolute right or wrong answers` \n",
    " \n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "<img src=\"Data Pipelines.png\"/>\n",
    "\n",
    "**<center>Data Pipelines Architecture Diagram</center>**\n",
    "\n",
    "Below are the steps necessary to pipeline the data into the chosen data model:\n",
    "1. Create a **Config**  file to store the **AWS credentials** which will be used in python code (env. variables) to access **AWS S3** bucket.\n",
    "2. Setup the **AWS S3 output files path**. (This path will serve a **Data Lake**).\n",
    "3. Create a **Spark session**.\n",
    "4. Perform the similar steps above but instead of using **Pandas**, we will use **PySpark** instead.\n",
    "5. Then we will read all files into the **Spark** dataframes, perfrom **cleaning** and **drop duplicate (if exists)**.\n",
    "6. Extract columns from those dataframes to create tables:\n",
    "    - 4.1 **Fact_H1B_Sponsors**. Note: These parquet files will be partitioned by **Work City & State**.\n",
    "    - 4.2 **Dim_Employers**\n",
    "    - 4.3 **Dim_Occupations**\n",
    "    - 4.4 **Dim_H1B_Cases**\n",
    "    - 4.5 **Dim_Job_Postings**\n",
    "7. Write thoese tables in the form of **parquet files**  and save to the output path.\n",
    "8. Check on the **ASW S3** if all thoese files are presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Below is the code for this end-to-end data pipeline to create the data model which previously designed.\n",
    "\n",
    "***\n",
    "Note 1: All the code can be used to create as an **ETL** python script file which can be run as a singleton to perform an end-to-end data pipeline processes.\n",
    "***\n",
    "\n",
    "***\n",
    "Note 2: My **output path** is **s3a://salinee-bucket/** which is my personel bucket that I already setup on **AWS** which will be my **Data Lake S3**.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "H1B Dataset count:4192087\n",
      "**************************************************\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Indeed Jobs Dataset count:30002\n",
      "**************************************************\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Amazon Jobs Dataset count:50\n",
      "**************************************************\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Cleaned H1B Dataset count:3838664\n",
      "**************************************************\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Cleaned Indeed Jobs Dataset count:30002\n",
      "**************************************************\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Cleaned Amazon Jobs Dataset count:46\n",
      "**************************************************\n",
      "\n",
      "\n",
      "******************************************************\n",
      "Finished reading \"dim_h1b_cases_table\" dataframe\n",
      "******************************************************\n",
      "\n",
      "\n",
      "******************************************************\n",
      "Finished reading \"dim_employers_table\" dataframe\n",
      "******************************************************\n",
      "\n",
      "\n",
      "******************************************************\n",
      "Finished reading \"dim_occupations_table\" dataframe\n",
      "******************************************************\n",
      "\n",
      "\n",
      "******************************************************\n",
      "Finished reading \"dim_job_postings_table\" dataframe\n",
      "******************************************************\n",
      "\n",
      "\n",
      "******************************************************\n",
      "Finished reading \"fact_h1b_sponsors_table\" dataframe\n",
      "******************************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('credentials.cfg')\n",
    "\n",
    "# load AWS Credentials\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "# Set up Output Path for the write\n",
    "output_data = \"output/{}\"\n",
    "# output_data = \"s3a://salinee-bucket/{}\"\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# When the value of mapreduce.fileoutputcommitter.algorithm.version is 2, \n",
    "# task moves data generated by a task directly to the final destination\n",
    "# which make the writing process much faster\n",
    "spark.conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "\n",
    "# Set this configuration to restore the behavior before Spark 3.0 since our raw datetime format is in M/dd/yyyy format\n",
    "# which consider old format\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "# Since our raw datetime is in an old format, set this up will write parquet files as is without throwing an error\n",
    "spark.sql(\"set spark.sql.legacy.parquet.datetimeRebaseModeInWrite=CORRECTED\")\n",
    "\n",
    "## -- Read all data from files -- ##\n",
    "# Read H1B data from file into the dataframe\n",
    "h1b_df = spark.read.format(\"csv\")\\\n",
    "         .option(\"header\", \"true\")\\\n",
    "         .load(\"h1b_data_fy2011_fy2018_20190401.csv\")\n",
    "\n",
    "\n",
    "# Read Indeed Job Post data from file into the dataframe\n",
    "indeed_jobs_df = spark.read.format(\"csv\")\\\n",
    "                 .option(\"header\", \"true\")\\\n",
    "                 .load(\"marketing_sample_for_trulia_com-real_estate__20190901_20191031__30k_data.csv\")\n",
    "\n",
    "# Read Amazon Job Post data from file into the dataframe\n",
    "xtree = et.parse(\"amazon_com-jobs__20190901_20191231_sample.xml\")\n",
    "xroot = xtree.getroot()\n",
    "\n",
    "amazon_jobs_row = Row(\"uniq_id\", \"crawl_timestamp\", \"job_url\", \"title\", \"description\", \"location\", \\\n",
    "                       \"city\", \"state\", \"country\", \"category\", \"job_id\", \"company_name\")\n",
    "amazon_jobs_rows = []\n",
    "\n",
    "# We interested in the Page node and all of it's elements\n",
    "for page in xroot.findall('page'):\n",
    "    for record in page.findall('record'):\n",
    "        s_uniq_id = record.find(\"uniq_id\").text\n",
    "        s_crawl_timestamp = record.find(\"crawl_timestamp\").text if record is not None else None\n",
    "        s_job_url = record.find(\"job_url\").text if record is not None else None\n",
    "        s_title = record.find(\"title\").text if record is not None else None\n",
    "        s_description = record.find(\"description\").text if record is not None else None\n",
    "        s_location = record.find(\"location\").text if record is not None else None\n",
    "        \n",
    "        # Extract country, city, state from location column\n",
    "        s_address = s_location.split(',')\n",
    "        if len(s_address) == 3:\n",
    "            # This address has complete Country, State and City\n",
    "            s_country = s_address[0]\n",
    "            s_state = s_address[1]\n",
    "            s_city = s_address[2]\n",
    "        elif len(s_address) == 2:\n",
    "            # This address has only Country and City (probably not in the U.S.)\n",
    "            s_country = s_address[0]\n",
    "            s_state = None\n",
    "            s_city = s_address[1]\n",
    "        elif len(s_address) == 1:\n",
    "            # This address has only Country\n",
    "            s_country = s_address[0]\n",
    "            s_state = None\n",
    "            s_city = None\n",
    "        s_category = record.find(\"category\").text if record is not None else None\n",
    "        s_job_id = record.find(\"job_id\").text if record is not None else None\n",
    "        s_company_name = record.find(\"company_name\").text if record is not None else None\n",
    "        \n",
    "        # Append the extract information into rows\n",
    "        amazon_jobs_rows.append(amazon_jobs_row(s_uniq_id, s_crawl_timestamp, s_job_url, s_title, s_description,\\\n",
    "                                s_location, s_city, s_state, s_country, s_category, s_job_id, s_company_name))\n",
    "amazon_jobs_df = spark.createDataFrame(amazon_jobs_rows)\n",
    "\n",
    "## -- Count dataframe's records (Before Cleaning) -- ##\n",
    "# H1B\n",
    "print('**************************************************')\n",
    "print(f'H1B Dataset count:{h1b_df.count()}')\n",
    "print('**************************************************')\n",
    "print('\\n')\n",
    "\n",
    "# Indeed Jobs\n",
    "print('**************************************************')\n",
    "print(f'Indeed Jobs Dataset count:{indeed_jobs_df.count()}')\n",
    "print('**************************************************')\n",
    "print('\\n')\n",
    "\n",
    "# Amazon Jobs\n",
    "print('**************************************************')\n",
    "print(f'Amazon Jobs Dataset count:{amazon_jobs_df.count()}')\n",
    "print('**************************************************')\n",
    "print('\\n')\n",
    "\n",
    "## -- Perform Data Cleaning -- ##\n",
    "# H1B\n",
    "# Drop records for null values on 'work_city' or 'work_state' (if exists)\n",
    "h1b_df_valid = h1b_df.filter(h1b_df.work_city.isNotNull() & h1b_df.work_state.isNotNull())\n",
    "# Drop duplicate records\n",
    "h1b_df_valid = h1b_df_valid.dropDuplicates()\n",
    "\n",
    "# Indeed Jobs\n",
    "# Drop records for null values on 'City' or 'State' (if exists)\n",
    "indeed_jobs_df_valid = indeed_jobs_df.filter(indeed_jobs_df.City.isNotNull() & indeed_jobs_df.State.isNotNull())\n",
    "# Drop duplicate records\n",
    "indeed_jobs_df_valid = indeed_jobs_df_valid.dropDuplicates()\n",
    "\n",
    "# Amazon Jobs\n",
    "# Drop records for null values on 'city' or 'state' (if exists)\n",
    "amazon_jobs_df_valid = amazon_jobs_df.filter(amazon_jobs_df.city.isNotNull() & amazon_jobs_df.state.isNotNull())\n",
    "# Drop duplicate records\n",
    "amazon_jobs_df_valid = amazon_jobs_df_valid.dropDuplicates()\n",
    "\n",
    "## -- Count dataframe's records (After Cleaning) -- ##\n",
    "# H1B\n",
    "print('**************************************************')\n",
    "print(f'Cleaned H1B Dataset count:{h1b_df_valid.count()}')\n",
    "print('**************************************************')\n",
    "print('\\n')\n",
    "\n",
    "# Indeed Jobs\n",
    "print('**************************************************')\n",
    "print(f'Cleaned Indeed Jobs Dataset count:{indeed_jobs_df_valid.count()}')\n",
    "print('**************************************************')\n",
    "print('\\n')\n",
    "\n",
    "# Amazon Jobs\n",
    "print('**************************************************')\n",
    "print(f'Cleaned Amazon Jobs Dataset count:{amazon_jobs_df_valid.count()}')\n",
    "print('**************************************************')\n",
    "print('\\n')\n",
    "\n",
    "## -- Extract columns to create tables designed in the Data Model diagram -- ##\n",
    "# Dim_H1B_Cases\n",
    "# Note1: need to cast 'case_submitted' & 'decision_date' from String to Date type\n",
    "# Note2: Add an id column by auto increment this column\n",
    "dim_h1b_cases_table = h1b_df_valid.select(monotonically_increasing_id().alias(\"id\"), \\\n",
    "                               to_date(from_unixtime(unix_timestamp('case_submitted', 'M/dd/yyyy'))).alias('case_submitted'),\\\n",
    "                               to_date(from_unixtime(unix_timestamp('decision_date', 'M/dd/yyyy'))).alias(\"decision_date\"), \\\n",
    "                               col(\"job_title\"),\\\n",
    "                               col(\"work_city\"),\\\n",
    "                               col(\"work_state\")).dropDuplicates()\n",
    "\n",
    "print('******************************************************')\n",
    "print(f'Finished reading \"dim_h1b_cases_table\" dataframe')\n",
    "print('******************************************************')\n",
    "print('\\n')\n",
    "\n",
    "# Dim_Employers\n",
    "# Note1: Add an id column by auto increment this column\n",
    "dim_employers_table = h1b_df_valid.select(monotonically_increasing_id().alias(\"id\"), \\\n",
    "                                    col(\"emp_name\").alias(\"employer_name\"),\\\n",
    "                                    col(\"emp_city\").alias(\"employer_city\"),\\\n",
    "                                    col(\"emp_state\").alias(\"employer_state\")).dropDuplicates()\n",
    "\n",
    "print('******************************************************')\n",
    "print(f'Finished reading \"dim_employers_table\" dataframe')\n",
    "print('******************************************************')\n",
    "print('\\n')\n",
    "\n",
    "# Dim_Occupations\n",
    "# Note: Add an id column by auto increment this column\n",
    "dim_occupations_table = h1b_df_valid.select(monotonically_increasing_id().alias(\"id\"), \\\n",
    "                                    col(\"soc_code\"),\\\n",
    "                                    col(\"soc_name\"),\\\n",
    "                                    col(\"naics_code\"),\\\n",
    "                                    col(\"full_time_position\").cast(IntegerType()).alias(\"full_time\"),\\\n",
    "                                    col(\"prevailing_wage\").cast(FloatType()).alias(\"prevailing_wage\"), \\\n",
    "                                    col(\"pw_unit\").alias(\"prevailing_unit\"), \\\n",
    "                                    col(\"wage_from\").cast(FloatType()).alias(\"wage_from\"), \\\n",
    "                                    col(\"wage_to\").cast(FloatType()).alias(\"wage_to\")).dropDuplicates()\n",
    "\n",
    "print('******************************************************')\n",
    "print(f'Finished reading \"dim_occupations_table\" dataframe')\n",
    "print('******************************************************')\n",
    "print('\\n')\n",
    "\n",
    "# Dim_Job_Postings\n",
    "# Extract columns from Indeed Job Posting dataframe\n",
    "indeed_job_postings = indeed_jobs_df_valid.select(col(\"Job Title\").alias(\"job_title\"),\\\n",
    "                                    col(\"City\").alias(\"job_city\"),\\\n",
    "                                    col(\"State\").alias(\"job_state\"),\\\n",
    "                                    col(\"Company Name\").alias(\"company_name\"),\\\n",
    "                                    col(\"Crawl Timestamp\").cast(TimestampType()).alias(\"crawl_timestamp\"))\n",
    "# Add new 'source' with constant value = 'Indeed' to inform where this data came from\n",
    "indeed_job_postings = indeed_job_postings.withColumn(\"source\", lit(\"Indeed\"))\n",
    "\n",
    "# Extract columns from Amazon Job Posting dataframe\n",
    "amazob_job_postings = amazon_jobs_df_valid.select(col(\"title\").alias(\"job_title\"),\\\n",
    "                                    col(\"city\").alias(\"job_city\"),\\\n",
    "                                    col(\"state\").alias(\"job_state\"),\\\n",
    "                                    col(\"company_name\").alias(\"company_name\"),\\\n",
    "                                    col(\"crawl_timestamp\").cast(TimestampType()).alias(\"crawl_timestamp\"))\n",
    "# Add new 'source' with constant value = 'Amazon' to inform where this data came from\n",
    "amazob_job_postings = amazob_job_postings.withColumn(\"source\", lit(\"Amazon\"))\n",
    "\n",
    "# Combine two dataframe into a new 'Job Postings' table\n",
    "dim_job_postings_table = indeed_job_postings.union(amazob_job_postings);\n",
    "\n",
    "# Add an id column by auto increment this column\n",
    "dim_job_postings_table = dim_job_postings_table.select(monotonically_increasing_id().alias(\"id\"), \\\n",
    "                    col(\"job_title\"),\\\n",
    "                    col(\"job_city\"),\\\n",
    "                    col(\"job_state\"),\\\n",
    "                    col(\"company_name\"),\\\n",
    "                    col(\"crawl_timestamp\"),\n",
    "                    col(\"source\")).dropDuplicates()\n",
    "\n",
    "print('******************************************************')\n",
    "print(f'Finished reading \"dim_job_postings_table\" dataframe')\n",
    "print('******************************************************')\n",
    "print('\\n')\n",
    "\n",
    "# Fact_H1B_Sponsors\n",
    "# Note1: I joined 'dim_job_postings_table' as a left outer join because \n",
    "#        we can't be sure if the H1B jobs have been advertised on job board's sites, \n",
    "#        we probably won't found a lot of matches\n",
    "# Note2: Add an id column by auto increment this column (on the last select statement)\n",
    "fact_h1b_sponsors_table = h1b_df_valid \\\n",
    "                          .join(dim_h1b_cases_table, \n",
    "                               (date_format(dim_h1b_cases_table.case_submitted, \"M/dd/yyyy\") == h1b_df_valid.case_submitted) & \n",
    "                               (date_format(dim_h1b_cases_table.decision_date, \"M/dd/yyyy\") == h1b_df_valid.decision_date) &\n",
    "                               (dim_h1b_cases_table.job_title == h1b_df_valid.job_title) & \n",
    "                               (dim_h1b_cases_table.work_city == h1b_df_valid.work_city) &\n",
    "                               (dim_h1b_cases_table.work_state == h1b_df_valid.work_state),\n",
    "                           'inner')\\\n",
    "                           .join(dim_occupations_table, \n",
    "                               (dim_occupations_table.soc_code == h1b_df_valid.soc_code) &\n",
    "                               (dim_occupations_table.soc_name == h1b_df_valid.soc_name) &\n",
    "                               (dim_occupations_table.naics_code == h1b_df_valid.naics_code) &\n",
    "                               (dim_occupations_table.full_time.cast(StringType()) == h1b_df_valid.full_time_position) & \n",
    "                               (dim_occupations_table.prevailing_wage.cast(StringType()) == h1b_df_valid.prevailing_wage) &\n",
    "                               (dim_occupations_table.prevailing_unit.cast(StringType())  == h1b_df_valid.pw_unit) &\n",
    "                               (dim_occupations_table.wage_from == h1b_df_valid.wage_from) &\n",
    "                               (dim_occupations_table.wage_to == h1b_df_valid.wage_to),\n",
    "                           'inner')\\\n",
    "                            .join(dim_employers_table, \n",
    "                               (dim_employers_table.employer_name == h1b_df_valid.emp_name) & \n",
    "                               (dim_employers_table.employer_city == h1b_df_valid.emp_city) &\n",
    "                               (dim_employers_table.employer_state == h1b_df_valid.emp_state),\n",
    "                           'inner')\\\n",
    "                            .join(dim_job_postings_table, \n",
    "                               (dim_job_postings_table.job_title == h1b_df_valid.job_title) & \n",
    "                               (dim_job_postings_table.job_city == h1b_df_valid.work_city) &\n",
    "                               (dim_job_postings_table.job_state == h1b_df_valid.work_state) & \n",
    "                               (dim_job_postings_table.company_name == h1b_df_valid.emp_name),\n",
    "                           'left_outer')\\\n",
    "                             .select(monotonically_increasing_id().alias(\"id\"),\\\n",
    "                                     dim_h1b_cases_table.id.alias(\"h1b_id\"),\\\n",
    "                                     dim_occupations_table.id.alias(\"occupation_id\"),\\\n",
    "                                     dim_employers_table.id.alias(\"employer_id\"),\\\n",
    "                                     dim_job_postings_table.id.alias(\"job_posting_id\"),\\\n",
    "                                     dim_h1b_cases_table.work_city.alias(\"work_city\"),\\\n",
    "                                     dim_h1b_cases_table.work_state.alias(\"work_state\"))\n",
    "\n",
    "print('******************************************************')\n",
    "print(f'Finished reading \"fact_h1b_sponsors_table\" dataframe')\n",
    "print('******************************************************')\n",
    "print('\\n')\n",
    "\n",
    "## -- Write all tables to parquet files -- ##\n",
    "# Dim_H1B_Cases\n",
    "dim_h1b_cases_table.write.mode(\"overwrite\")\\\n",
    "                    .parquet(output_data.format(\"dim_h1b_cases_table\"))\n",
    "print('******************************************************')\n",
    "print(f'Finished writing \"dim_h1b_cases_table\" parquet files')\n",
    "print('******************************************************')\n",
    "print('\\n')\n",
    "\n",
    "# Dim_Employers\n",
    "dim_employers_table.write.mode(\"overwrite\")\\\n",
    "                    .parquet(output_data.format(\"dim_employers_table\"))\n",
    "print('******************************************************')\n",
    "print(f'Finished writing \"dim_employers_table\" parquet files')\n",
    "print('******************************************************')\n",
    "print('\\n')\n",
    "\n",
    "# Dim_Occupations\n",
    "dim_occupations_table.write.mode(\"overwrite\")\\\n",
    "                    .parquet(output_data.format(\"dim_occupations_table\"))\n",
    "print('******************************************************')\n",
    "print(f'Finished writing \"dim_occupations_table\" parquet files')\n",
    "print('******************************************************')\n",
    "print('\\n')\n",
    "      \n",
    "# Dim_Job_Postings\n",
    "dim_job_postings_table.write.mode(\"overwrite\")\\\n",
    "                    .parquet(output_data.format(\"dim_job_postings_table\"))\n",
    "print('******************************************************')\n",
    "print(f'Finished writing \"dim_job_postings_table\" parquet files')\n",
    "print('******************************************************')\n",
    "print('\\n')\n",
    "      \n",
    "# Fact_H1B_Sponsors\n",
    "# Note: Partitioned by 'work_city' and 'work_state'\n",
    "fact_h1b_sponsors_table.write.mode(\"overwrite\")\\\n",
    ".partitionBy(\"work_city\", \"work_state\")\\\n",
    ".parquet(output_data.format(\"fact_h1b_sponsors_table\"))\n",
    "print('******************************************************')\n",
    "print(f'Finished writing \"fact_h1b_sponsors_table\" parquet files')\n",
    "print('******************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "The steps to perform here to ensure the pipeline ran as expected are:\n",
    " * **Check Schema** for each tables to make sure all columns have the correct data types as intended.\n",
    " * **Count Check** for each tables to make sure all tables has some records inserted. (Not an empty table)\n",
    " * **Check Null on the Primary Key** of each tables to make sure there is no corrupted records for all the tables.\n",
    " \n",
    " First, we will check what is the original schema for each tables. We will use these schemas to compare against the dafaframe from **parquet files** and the schema for the same table should look the same regardless of the sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- case_submitted: date (nullable = true)\n",
      " |-- decision_date: date (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- work_city: string (nullable = true)\n",
      " |-- work_state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Schema for 'Dim_H1B_Cases' table\n",
    "dim_h1b_cases_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- employer_name: string (nullable = true)\n",
      " |-- employer_city: string (nullable = true)\n",
      " |-- employer_state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Schema for 'Dim_Employers' table\n",
    "dim_employers_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- soc_code: string (nullable = true)\n",
      " |-- soc_name: string (nullable = true)\n",
      " |-- naics_code: string (nullable = true)\n",
      " |-- full_time: integer (nullable = true)\n",
      " |-- prevailing_wage: float (nullable = true)\n",
      " |-- prevailing_unit: string (nullable = true)\n",
      " |-- wage_from: float (nullable = true)\n",
      " |-- wage_to: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Schema for 'Dim_Occupations' table\n",
    "dim_occupations_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- job_city: string (nullable = true)\n",
      " |-- job_state: string (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- crawl_timestamp: timestamp (nullable = true)\n",
      " |-- source: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Schema for 'Dim_Job_Postings' table\n",
    "dim_job_postings_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- h1b_id: long (nullable = false)\n",
      " |-- occupation_id: long (nullable = false)\n",
      " |-- employer_id: long (nullable = false)\n",
      " |-- job_posting_id: long (nullable = true)\n",
      " |-- work_city: string (nullable = true)\n",
      " |-- work_state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Schema for 'Fact_H1B_Sponsors' table\n",
    "fact_h1b_sponsors_table.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Quality Checks\n",
    "Below is the code for this end-to-end data quality checks processes.\n",
    "\n",
    "***\n",
    "Note: All the code can be used to create as an **Unit Test** python script file which can be run as a singleton.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************\n",
      "Finished reading all dataframes from parquet files\n",
      "****************************************************\n",
      "Schema quality on table \u001b[1mDim_H1B_Cases.id\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_H1B_Cases.case_submitted\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_H1B_Cases.decision_date\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_H1B_Cases.job_title\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_H1B_Cases.work_city\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_H1B_Cases.work_state\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_Employers.id\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_Employers.employer_name\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_Employers.employer_city\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_Employers.employer_state\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_Occupations.id\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_Occupations.soc_code\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_Occupations.soc_name\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_Occupations.naics_code\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_Occupations.full_time\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_Occupations.prevailing_wage\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_Occupations.prevailing_unit\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_Occupations.wage_from\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_Occupations.wage_to\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_Job_Postings.id\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_Job_Postings.job_title\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_Job_Postings.job_city\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_Job_Postings.job_state\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_Job_Postings.company_name\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_Job_Postings.crawl_timestamp\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mDim_Job_Postings.source\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mFact_H1B_Sponsors.id\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mFact_H1B_Sponsors.h1b_id\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mFact_H1B_Sponsors.occupation_id\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mFact_H1B_Sponsors.employer_id\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mFact_H1B_Sponsors.job_posting_id\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mFact_H1B_Sponsors.work_city\u001b[0m field check passed. ✓\n",
      "Schema quality on table \u001b[1mFact_H1B_Sponsors.work_state\u001b[0m field check passed. ✓\n",
      "****************************************************************\n",
      "Finished checking schemas for all dataframes from parquet files\n",
      "****************************************************************\n",
      "Data quality on table \u001b[1mDim_H1B_Cases\u001b[0m check passed with 402935 records. ✓\n",
      "Data quality on table \u001b[1mDim_Employers\u001b[0m check passed with 402935 records. ✓\n",
      "Data quality on table \u001b[1mDim_Occupations\u001b[0m check passed with 402935 records. ✓\n",
      "Data quality on table \u001b[1mDim_Job_Postings\u001b[0m check passed with 30048 records. ✓\n",
      "Data quality on table \u001b[1mFact_H1B_Sponsors\u001b[0m check passed with 6867698 records. ✓\n",
      "****************************************************************\n",
      "Finished count checking for all dataframes from parquet files\n",
      "****************************************************************\n",
      "Data quality on table \u001b[1mDim_H1B_Cases\u001b[0m check passed. No record is corrupted. ✓\n",
      "Data quality on table \u001b[1mDim_Employers\u001b[0m check passed. No record is corrupted. ✓\n",
      "Data quality on table \u001b[1mDim_Occupations\u001b[0m check passed. No record is corrupted. ✓\n",
      "Data quality on table \u001b[1mDim_Job_Postings\u001b[0m check passed. No record is corrupted. ✓\n",
      "Data quality on table \u001b[1mFact_H1B_Sponsors\u001b[0m check passed. No record is corrupted. ✓\n",
      "***********************************************************************\n",
      "Finished checking NULL on the PK for all dataframes from parquet files\n",
      "***********************************************************************\n"
     ]
    }
   ],
   "source": [
    "## -- Define functions used in this unit test -- ##\n",
    "def checkSchema(dataframe, table_tuples, table_name):\n",
    "    \"\"\"This function will loop through the provided dataframe and check the datatype for each field name with the \n",
    "       field datatype to compared if it's the same as expected or not\n",
    "    Parameters: \n",
    "        dataframe: Spark DataFrame\n",
    "        table_tuples: list of expected 'field_name' and 'field_datatype'\n",
    "        table_name: string of the table's name that we're checking the schema\n",
    "    Returns: \n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loop through each dataframe's fields\n",
    "    for f in dataframe.schema.fields:\n",
    "        try:\n",
    "            # Find the expected field's datatype for the specific field's name\n",
    "            table_field = next(x for x in table_tuples if x[\"field_name\"] == f.name)\n",
    "            if (table_field != None):\n",
    "                # Check datatype for each fields to see if it has the same type as expected\n",
    "                if f.dataType != table_field[\"field_datatype\"]:\n",
    "                    # If Not, raise an exception\n",
    "                    raise ValueError(f'Schema quality check failed. \\033[1m{table_name}.{f.name}\\033[0m field has wrong data type. \\u2717')\n",
    "                else:\n",
    "                    # If yes, check passed\n",
    "                    print(f'Schema quality on table \\033[1m{table_name}.{f.name}\\033[0m field check passed. \\u2713')\n",
    "        except StopIteration as ex:\n",
    "            print(ex)\n",
    "            \n",
    "def checkCount(dataframe, table_name):\n",
    "    \"\"\"This function check if the provided dataframe has some records or not\n",
    "    Parameters: \n",
    "        dataframe: Spark DataFrame\n",
    "        table_name: string of the table's name that we're checking the schema\n",
    "    Returns: \n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Count the dataframe's records\n",
    "    counts = dataframe.count()\n",
    "    if counts < 1:\n",
    "        # If there is no records, raise an exception\n",
    "        raise ValueError(f'Data quality check failed. \\033[1m{table_name}\\033[0m table returned no results. \\u2717')\n",
    "    else:\n",
    "        # Otherwise, check passed\n",
    "        print(f'Data quality on table \\033[1m{table_name}\\033[0m check passed with {counts} records. \\u2713')\n",
    "        \n",
    "def checkNullOnPK(dataframe, pk_field_name, table_name):\n",
    "    \"\"\"This function check if the provided dataframe has some records or not\n",
    "    Parameters: \n",
    "        dataframe: Spark DataFrame\n",
    "        table_name: string of the table's name that we're checking the schema\n",
    "    Returns: \n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Count the dataframe's records where it's primary key column has NULL value\n",
    "    pk_null_counts = dataframe.where(dataframe[pk_field_name].isNull()).count()\n",
    "    \n",
    "    if pk_null_counts > 0:\n",
    "        # If there is some records, raise an exception\n",
    "        raise ValueError(f'Data quality check failed. \\033[1m{table_name}\\033[0m table returned some records with null PRIMARY KEY. \\u2717')\n",
    "    else:\n",
    "        # Otherwise, check passed\n",
    "        print(f'Data quality on table \\033[1m{table_name}\\033[0m check passed. No record is corrupted. \\u2713')\n",
    "\n",
    "## -- Read parquet files from each table's folders -- ##\n",
    "# Dim_H1B_Cases\n",
    "dim_h1b_cases_parquet_df = spark.read.parquet(output_data.format(\"dim_h1b_cases_table\"))\n",
    "\n",
    "# Dim_Employers\n",
    "dim_employers_parquet_df = spark.read.parquet(output_data.format(\"dim_employers_table\"))\n",
    "\n",
    "# Dim_Occupations\n",
    "dim_occupations_parquet_df = spark.read.parquet(output_data.format(\"dim_occupations_table\"))\n",
    "\n",
    "# Dim_Job_Postings\n",
    "dim_job_postings_parquet_df = spark.read.parquet(output_data.format(\"dim_job_postings_table\"))\n",
    "\n",
    "# Fact_H1B_Sponsors\n",
    "fact_h1b_sponsors_parquet_df = spark.read.parquet(output_data.format(\"fact_h1b_sponsors_table\"))\n",
    "\n",
    "print('***************************************************')\n",
    "print(f'Finished reading all dataframes from parquet files')\n",
    "print('****************************************************')\n",
    "\n",
    "## -- Perform quality checks -- ##\n",
    "##################################\n",
    "\n",
    "## -- 1. Check Schema -- ##\n",
    "# Dim_H1B_Cases\n",
    "table_tuples = []\n",
    "\n",
    "table_tuples.append({'field_name': 'id', 'field_datatype': LongType()})\n",
    "table_tuples.append({'field_name': 'case_submitted', 'field_datatype': DateType()})\n",
    "table_tuples.append({'field_name': 'decision_date', 'field_datatype': DateType()})\n",
    "table_tuples.append({'field_name': 'job_title', 'field_datatype': StringType()})\n",
    "table_tuples.append({'field_name': 'work_city', 'field_datatype': StringType()})\n",
    "table_tuples.append({'field_name': 'work_state', 'field_datatype': StringType()})\n",
    "\n",
    "checkSchema(dim_h1b_cases_parquet_df, table_tuples, 'Dim_H1B_Cases')\n",
    "\n",
    "# Dim_Employers\n",
    "table_tuples = []\n",
    "\n",
    "table_tuples.append({'field_name': 'id', 'field_datatype': LongType()})\n",
    "table_tuples.append({'field_name': 'employer_name', 'field_datatype': StringType()})\n",
    "table_tuples.append({'field_name': 'employer_city', 'field_datatype': StringType()})\n",
    "table_tuples.append({'field_name': 'employer_state', 'field_datatype': StringType()})\n",
    "\n",
    "checkSchema(dim_employers_parquet_df, table_tuples, 'Dim_Employers')\n",
    "            \n",
    "# Dim_Occupations\n",
    "table_tuples = []\n",
    "\n",
    "table_tuples.append({'field_name': 'id', 'field_datatype': LongType()})\n",
    "table_tuples.append({'field_name': 'full_time', 'field_datatype': IntegerType()})\n",
    "table_tuples.append({'field_name': 'prevailing_wage', 'field_datatype': FloatType()})\n",
    "table_tuples.append({'field_name': 'wage_from', 'field_datatype': FloatType()})\n",
    "table_tuples.append({'field_name': 'wage_to', 'field_datatype': FloatType()})\n",
    "table_tuples.append({'field_name': 'soc_code', 'field_datatype': StringType()})\n",
    "table_tuples.append({'field_name': 'soc_name', 'field_datatype': StringType()})\n",
    "table_tuples.append({'field_name': 'naics_code', 'field_datatype': StringType()})\n",
    "table_tuples.append({'field_name': 'prevailing_unit', 'field_datatype': StringType()})\n",
    "\n",
    "checkSchema(dim_occupations_parquet_df, table_tuples, 'Dim_Occupations')\n",
    "            \n",
    "# Dim_Job_Postings\n",
    "table_tuples = []\n",
    "\n",
    "table_tuples.append({'field_name': 'id', 'field_datatype': LongType()})\n",
    "table_tuples.append({'field_name': 'crawl_timestamp', 'field_datatype': TimestampType()})\n",
    "table_tuples.append({'field_name': 'job_title', 'field_datatype': StringType()})\n",
    "table_tuples.append({'field_name': 'job_city', 'field_datatype': StringType()})\n",
    "table_tuples.append({'field_name': 'job_state', 'field_datatype': StringType()})\n",
    "table_tuples.append({'field_name': 'company_name', 'field_datatype': StringType()})\n",
    "table_tuples.append({'field_name': 'source', 'field_datatype': StringType()})\n",
    "\n",
    "checkSchema(dim_job_postings_parquet_df, table_tuples, 'Dim_Job_Postings')\n",
    "            \n",
    "# Fact_H1B_Sponsors\n",
    "table_tuples = []\n",
    "\n",
    "table_tuples.append({'field_name': 'id', 'field_datatype': LongType()})\n",
    "table_tuples.append({'field_name': 'h1b_id', 'field_datatype': LongType()})\n",
    "table_tuples.append({'field_name': 'occupation_id', 'field_datatype': LongType()})\n",
    "table_tuples.append({'field_name': 'employer_id', 'field_datatype': LongType()})\n",
    "table_tuples.append({'field_name': 'job_posting_id', 'field_datatype': LongType()})\n",
    "table_tuples.append({'field_name': 'work_city', 'field_datatype': StringType()})\n",
    "table_tuples.append({'field_name': 'work_state', 'field_datatype': StringType()})\n",
    "\n",
    "checkSchema(fact_h1b_sponsors_parquet_df, table_tuples, 'Fact_H1B_Sponsors')\n",
    "\n",
    "            \n",
    "print('****************************************************************')\n",
    "print(f'Finished checking schemas for all dataframes from parquet files')\n",
    "print('****************************************************************')\n",
    "\n",
    "## -- 2. Count Check -- ##\n",
    "\n",
    "# Dim_H1B_Cases\n",
    "checkCount(dim_h1b_cases_parquet_df, 'Dim_H1B_Cases')\n",
    "\n",
    "# Dim_Employers\n",
    "checkCount(dim_employers_parquet_df, 'Dim_Employers')\n",
    "\n",
    "# Dim_Occupations\n",
    "checkCount(dim_occupations_parquet_df, 'Dim_Occupations')\n",
    "\n",
    "# Dim_Job_Postings\n",
    "checkCount(dim_job_postings_parquet_df, 'Dim_Job_Postings')\n",
    "\n",
    "# Fact_H1B_Sponsors\n",
    "checkCount(fact_h1b_sponsors_parquet_df, 'Fact_H1B_Sponsors')\n",
    "\n",
    "print('****************************************************************')\n",
    "print(f'Finished count checking for all dataframes from parquet files')\n",
    "print('****************************************************************')\n",
    "\n",
    "## -- 3. Check Null on the Primary Key -- ##\n",
    "\n",
    "# Dim_H1B_Cases\n",
    "checkNullOnPK(dim_h1b_cases_parquet_df, \"id\", \"Dim_H1B_Cases\")\n",
    "\n",
    "# Dim_Employers\n",
    "checkNullOnPK(dim_employers_parquet_df, \"id\", \"Dim_Employers\")\n",
    "\n",
    "# Dim_Occupations\n",
    "checkNullOnPK(dim_occupations_parquet_df, \"id\", \"Dim_Occupations\")\n",
    "\n",
    "# Dim_Job_Postings\n",
    "checkNullOnPK(dim_job_postings_parquet_df, \"id\", \"Dim_Job_Postings\")\n",
    "\n",
    "# Fact_H1B_Sponsors\n",
    "checkNullOnPK(fact_h1b_sponsors_parquet_df, \"id\", \"Fact_H1B_Sponsors\")\n",
    "\n",
    "print('***********************************************************************')\n",
    "print(f'Finished checking NULL on the PK for all dataframes from parquet files')\n",
    "print('***********************************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary\n",
    "Below are the **data dictionary** for each tables in the data model for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Dim_H1B_Cases\n",
    "\n",
    "| Field Name | Data Type | Description | Required? | Accepts null value? | Notes |\n",
    "|------|------|------|------|------|------|\n",
    "|   id  | long | Identity column | Yes | No | Primary Key |\n",
    "|   case_submitted  | date | Date when the H1B case was submit to the system | No | Yes |  |\n",
    "|   decision_date  | date | Date when the H1B case's decision was made | No | Yes |  |\n",
    "|   job_title | string | Job title when filing the H1B's case | No | Yes | |\n",
    "|   work_city  | string | City to perform the work when filing the H1B's case | No | Yes | |\n",
    "|   work_state  | string | State to perform the work when filing the H1B's case | No | Yes | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 Dim_Employers\n",
    "\n",
    "| Field Name | Data Type | Description | Required? | Accepts null value? | Notes |\n",
    "|------|------|------|------|------|------|\n",
    "|   id  | long | Identity column | Yes | No | Primary Key |\n",
    "|   employer_name  | string | Employer's name | No | Yes |  |\n",
    "|   employer_city  | string | City where the employer's company resides | No | Yes |  |\n",
    "|   employer_state  | string | State where the employer's company resides | No | Yes |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 Dim_Occupations\n",
    "\n",
    "| Field Name | Data Type | Description | Required? | Accepts null value? | Notes |\n",
    "|------|------|------|------|------|------|\n",
    "|   id  | long | Identity column | Yes | No | Primary Key |\n",
    "|   soc_code  | string | Standard Occupational Code | No | Yes |  |\n",
    "|   soc_name | string | SOC occupation title | No | Yes |  |\n",
    "|   naics_code  | string | North American Industry Classification System (NAICS) | No | Yes |  |\n",
    "|   full_time  | integer | Job position fulltime's status | No | Yes |  |\n",
    "|   prevailing_wage | float | Average wage paid to similarly employed workers in a specific occupation | No | Yes |  |\n",
    "|   prevailing_unit | string | Prevailing Wage's unit of payment | No | Yes | For examle: by year, month, hour |\n",
    "|   wage_from  | float | Minimum (staring) wage for the job title | No | Yes |  |\n",
    "|   wage_to | float | Maximum wage for the job title | No | Yes |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.4 Dim_Job_Postings \n",
    "\n",
    "| Field Name | Data Type | Description | Required? | Accepts null value? | Notes |\n",
    "|------|------|------|------|------|------|\n",
    "|   id  | long | Identity column | Yes | No | Primary key |\n",
    "|   job_title | string | Job title | No | Yes |  |\n",
    "|   job_city | string | City where the job will be performed | No | Yes |  |\n",
    "|   job_state | string | State where the job will be performed | No | Yes |  |\n",
    "|   company_name | string | Job posted by company | No | Yes |  |\n",
    "|   crawl_timestamp | timestamp | The automatic crawler detect timestamp |  No | Yes |  |\n",
    "|   source | string | Source domain of where the jobs are posting | Yes | No |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.5 Fact_H1B_Sponsors \n",
    "\n",
    "| Field Name | Data Type | Description | Required? | Accepts null value? | Notes |\n",
    "|------|------|------|------|------|------|\n",
    "|   id | long | Identity column | Yes | No | Primary key |\n",
    "|   h1b_id | long | Reference key to **Dim_H1B_Cases** table | Yes | No | Foreign key |\n",
    "|   occupation_id | long | Reference key to **Dim_Occupations** table | Yes | No | Foreign key |\n",
    "|   employer_id  | long | Reference key to **Dim_Employers** table | Yes | No | Foreign key |\n",
    "|   job_posting_id  | long | Reference key to **Dim_Job_Postings** table | No | Yes | Nullable foreign key |\n",
    "|   work_city  | string | City where the job will be perform | No | Yes | Partition key |\n",
    "|   work_state  | string | State where the job will be perform | No | Yes | Partition key |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Clearly state the rationale for the choice of tools and technologies for the project.</font>\n",
    "\n",
    "**Below is the list of tools I decide to  used in this Projects:**\n",
    "- AWS regular account\n",
    "- AWS S3\n",
    "- Jupyter Notebook\n",
    "- Spark (built-in local mode)\n",
    "\n",
    "There are some reasons which make me decide to choose to implement the above technologies into this project:\n",
    "1. The functionality of the **AWS** cloud platform and the technology behind it have been finely tuned by many years and it's is very reliable and widely accepted by many industries.\n",
    "2. Due to the rising popularity of open-source software in the industry, along with rapid growth of data science and machine learning the **Jupyter Notebook** has become famous amoung many data scientists. We can easily implement many **AI & ML** related libraries for good useses. The notebook itself also easily to illustrate images, color, many graphics such as tables.  \n",
    "3. **Spark** is optimised to operate in-memory, allowing it to process data much quicker than other solutions which is good fit for project prototyping. (Good fit for development process, in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Propose how often the data should be updated and why.</font>\n",
    "\n",
    "We could update our data every 6 months (**Biyearly** basis). USCIS are are often taking 4-5 months to process and update the H-1B Petition's case status.[1] Therefore, I propose that 6 months timeframe is appropriate to update our data.\n",
    "\n",
    "<i>Source: https://internationalaffairs.uchicago.edu/page/processing-times-h-1b-petitions</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a description of how you would approach the problem differently under the following scenarios:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <font color=blue>The data was increased by 100x.</font>\n",
    "\n",
    "We can migrate from an existing **Data Lake with S3** to **AWS EMR** instead.\n",
    "\n",
    "**AWS EMR** has an **auto scaling** and **resizing** capabilities which is very flexible to increase/decrease data storage's sizes on demand which in the end, will reduce the overall costs since we don't need to rent the massive data storage all the time but Amazon will manage and calculate the data storage system for us.\n",
    "\n",
    "The downside of using **AWS EMR** is that it's **transient cluster** which means it is the compute clusters that automatically shut down when processing is finished. The data on the **AWS EMR** isn't real-time and it's require some kind of scheduler automation to update the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <font color=blue>The data populates a dashboard that must be updated on a daily basis by 7am every day.</font>\n",
    "\n",
    "We can integrate our code to work with **Apache Airflow** which is an open-source workflow management platform. Below is a sample code of how we setup the schedule interval on **DAG** that will run on a daily basis by 7am every day. \n",
    "\n",
    "```python\n",
    "dag = DAG('udac_example_dag',\n",
    "          description='Sample DAG with Airflow',\n",
    "          schedule_interval='0 0 7 1/1 * ? *'\n",
    "        )\n",
    "```\n",
    "**Cron expression: 0 0 7 1/1 * ? *** will generated the below scheduler: <I>(Show 5 intervals as an example)</I>\n",
    "1.\t2020-09-20 Sun 07:00:00\n",
    "2.\t2020-09-21 Mon 07:00:00\n",
    "3.\t2020-09-22 Tue 07:00:00\n",
    "4.\t2020-09-23 Wed 07:00:00\n",
    "5.\t2020-09-24 Thu 07:00:00\n",
    "***\n",
    "Note: Cron string was generated from http://www.cronmaker.com/.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <font color=blue>The database needed to be accessed by 100+ people.</font>\n",
    "\n",
    "We can migrate from an existing **Data Lake with S3** to **Data Warehouse with AWS Redshift** instead.\n",
    "\n",
    "**AWS Redshift**'s performance is outperform **S3 Bucket** when accessing data by large amount of users. With AWS Redshift, when it comes to queries that are executed frequently, the subsequent queries are usually executed faster. This is because Redshift spends a good portion of the execution plan optimizing the query.\n",
    "\n",
    "**AWS Redshift** has an architecture that allows massively parallel processing using multiple nodes, reducing the load times. **AWS Redshift**'s clusters are available **24X7** which is pretty convenient for 100+ people to access the data warehouse at anytime.\n",
    "\n",
    "The downside of using **AWS Redshift** is that it's generally costly compared with most of the **AWS Cloud** products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
