import configparser
from datetime import datetime
import os

from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col, monotonically_increasing_id
from pyspark.sql.types import TimestampType
from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format
from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date, LongType as Long


config = configparser.ConfigParser()
config.read('dl.cfg')

os.environ['AWS_ACCESS_KEY_ID']=config['AWS_ACCESS_KEY_ID']
os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS_SECRET_ACCESS_KEY']


def create_spark_session():
    """Create a Spark session in order to process the data
    Parameters:
        None
    Returns:
        Spark session.
    """ 
    
     # Create Spark Session
    spark = SparkSession \
            .builder \
            .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:2.7.0") \
            .getOrCreate()

    # When the value of mapreduce.fileoutputcommitter.algorithm.version is 2, 
    # task moves data generated by a task directly to the final destination
    # which make the writing process much faster
    spark.conf.set("mapreduce.fileoutputcommitter.algorithm.version", "2")
    
    return spark


def process_song_data(spark, input_data, output_data):
    """This function will read JSON input files from the provided path and extracting the data to be inserted into two tables:
        - songs (will store the queried data into a folder called songs_table)
        - artists (will store the queried data into a folder called artists_table)
    Parameters: 
        spark: Spark session object
        input_data: path of the inputdata to be processed
        output_data: path of the location to store the parquet files
    Returns: 
        None
    """
    
    # get filepath to song data file
    song_data = input_data.format("song_data/*/*/*/*.json")
    
    # read song data file
    df = spark.read.json(song_data, schema=songsSchema)
    
    # extract columns to create songs table
    songs_table = df.select(["song_id", "title", "artist_id", "year", "duration"]).dropDuplicates(["song_id"])
    
    # write songs table to parquet files partitioned by year and artist
    songs_table.write.mode("overwrite").partitionBy("year", "artist_id")\
    .parquet(output_data.format("songs_table"))

    # extract columns to create artists table
    artists_table = df.select(["artist_id",\
                               col("artist_name").alias("name"), 
                               col("artist_location").alias("location"),
                               col("artist_latitude").alias("latitude"),
                               col("artist_longitude").alias("longitude")])\
                    .dropDuplicates(["artist_id"])
    
    # write artists table to parquet files
    artists_table.write.mode("overwrite")\
    .parquet(output_data.format("artists_table"))


def process_log_data(spark, input_data, output_data):
    """This function will read JSON input files from the provided path and extracting the data to be inserted into three tables:
        - users (will store the queried data into a folder called users_table)
        - time (will store the queried data into a folder called time_table)
        - songplays (will store the queried data into a folder called songplays_table)
    Parameters: 
        spark: Spark session object
        input_data: path of the inputdata to be processed
        output_data: path of the location to store the parquet files
    Returns: 
        None
    """
    
    # get filepath to log data file
    log_data = input_data.format("log-data/*.json")

    # read log data file
    df = spark.read.json(log_data, schema=logsSchema)
    
    # filter by actions for song plays
    df = df[df['page'] == 'NextSong']

    # extract columns for users table    
    users_table = df.select([col("userId").alias("user_id"),
                             col("firstName").alias("first_name"), 
                             col("lastName").alias("last_name"),
                             "gender",
                             "level"])\
                    .dropDuplicates(["user_id"])
    
    # write users table to parquet files
    users_table.write.mode("overwrite")\
    .parquet(output_data.format("users_table"))

    # create timestamp column from original timestamp column
    get_timestamp = udf(lambda x: datetime.fromtimestamp(x/1000.0), TimestampType())
    df = df.withColumn("timestamp", get_timestamp(df['ts']))
    
    # create datetime column from original timestamp column
    get_datetime = udf(lambda x: datetime.fromtimestamp(x/1000.0), TimestampType())
    df = df.withColumn("datetime", get_datetime(df['ts']))
    
    # extract columns to create time table
    time_table = df.select([col("datetime").alias("start_time"),
                            hour(df["datetime"]).alias("hour"), 
                            dayofmonth(df["datetime"]).alias("day"), 
                            weekofyear(df["datetime"]).alias("week"), 
                            month(df["datetime"]).alias("month"), 
                            year(df["datetime"]).alias("year"), 
                            date_format(df["datetime"], "E").alias("weekday")])\
                    .dropDuplicates(["start_time"])
    
    # write time table to parquet files partitioned by year and month
    time_table.write.mode("overwrite").partitionBy("year", "month")\
   .parquet(output_data.format("time_table"))

    # read in song data to use for songplays table
    song_df = spark.read.json(input_data.format("song_data/*/*/*/*.json"), schema=songsSchema)

    # create songplays table 
    songplays_table = df.select(["song",
                                 "artist",
                                  col("datetime").alias("start_time"),
                                  col("userId").alias("user_id"), 
                                  "level", 
                                  col("sessionId").alias("session_id"), 
                                  "location",
                                 "length",
                                  col("userAgent").alias("user_agent")])
    # joined song and log datasets 
    songplays_table = songplays_table\
                      .join(song_df, (songplays_table.song == song_df.title) & 
                            (songplays_table.artist == song_df.artist_name) &
                            (songplays_table.length == song_df.duration), 'left_outer')
    # add a songplay_id column by auto increment this column
    songplays_table = songplays_table.withColumn("songplay_id", \
                                      monotonically_increasing_id())
    # extract columns to create songplays table 
    songplays_table = songplays_table.select(["songplay_id", "start_time", "user_id", "level", "song_id", "artist_id", "session_id", "location", "user_agent"])

    # write songplays table to parquet files partitioned by year and month
    songplays_table.withColumn("year",year('start_time'))\
                    .withColumn("month",month('start_time'))\
                    .write.mode("overwrite")\
                    .partitionBy("year", "month")\
                    .parquet(output_data.format("songplays_table"))


def main():
    # set up Spark session
    spark = create_spark_session()
    # set input & output S3's path
    input_data = "s3a://udacity-dend/{}"
    output_data = "s3a://salinee-bucket/{}"
    # process song data
    process_song_data(spark, input_data, output_data)   
    # process log data
    process_log_data(spark, input_data, output_data)


if __name__ == "__main__":
    main()
