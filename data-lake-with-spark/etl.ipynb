{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Develop ETL Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### The purpose of this notebook is to develop the ETL processes step-by-steps for this Data Lake project in order to extracts the data from S3, processes them using Spark, and loads the data back into S3 as a set of dimensional tables.\n",
    "\n",
    "**General Guidelines:**\n",
    "- We will read from a small subset of data instead of S3 for the input data source during this development phase.\n",
    "- We will write to our workstation instead of S3 during this development phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import all necessary libraries\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date, LongType as Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "# load AWS Credentials\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# set up Input & Output Path for the read/write\n",
    "input_data = \"data/{}\" \n",
    "output_data = \"output/{}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Check Schema for the Song data from the sample data source\n",
    "***\n",
    "**Note:**\n",
    "To improve the reading performance, we need to pre-defined the schema of the data instead of letting Spark figure it out on the fly.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read song data from the sample input data files\n",
    "song_data_df = spark.read.json(input_data.format(\"song-data/*/*/*/*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print Schema\n",
    "song_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# set up the Song data's schema which will use later for reading\n",
    "songsSchema = R([\n",
    "    Fld(\"artist_id\",Str()),\n",
    "    Fld(\"artist_latitude\",Dbl()),\n",
    "    Fld(\"artist_location\",Str()),\n",
    "    Fld(\"artist_longitude\",Dbl()),\n",
    "    Fld(\"artist_name\",Str()),\n",
    "    Fld(\"duration\",Dbl()),\n",
    "    Fld(\"num_songs\",Long()),\n",
    "    Fld(\"song_id\",Str()),\n",
    "    Fld(\"title\",Str()),\n",
    "    Fld(\"year\",Long())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read log data from the sample input data files\n",
    "log_data_df = spark.read.json(input_data.format(\"log-data/*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print Schema\n",
    "log_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# set up the log data's schema which will use later when process the log data's files\n",
    "logsSchema = R([\n",
    "    Fld(\"artist\",Str()),\n",
    "    Fld(\"auth\",Str()),\n",
    "    Fld(\"firstName\",Str()),\n",
    "    Fld(\"gender\",Str()),\n",
    "    Fld(\"itemInSession\",Long()),\n",
    "    Fld(\"lastName\",Str()),\n",
    "    Fld(\"length\",Dbl()),\n",
    "    Fld(\"level\",Str()),\n",
    "    Fld(\"location\",Str()),\n",
    "    Fld(\"method\",Str()),\n",
    "    Fld(\"page\",Str()),\n",
    "    Fld(\"registration\",Dbl()),\n",
    "    Fld(\"sessionId\",Long()),\n",
    "    Fld(\"song\",Str()),\n",
    "    Fld(\"status\",Long()),\n",
    "    Fld(\"ts\",Long()),\n",
    "    Fld(\"userAgent\",Str()),\n",
    "    Fld(\"userId\",Str())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"Create a Spark session in order to process the data\n",
    "    Parameters:\n",
    "        None\n",
    "    Returns:\n",
    "        Spark session.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create Spark Session\n",
    "    spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "    # When the value of mapreduce.fileoutputcommitter.algorithm.version is 2, \n",
    "    # task moves data generated by a task directly to the final destination\n",
    "    # which make the writing process much faster\n",
    "    spark.conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    \"\"\"This function will read JSON input files from the provided path and extracting the data to be inserted into two tables:\n",
    "        - songs (will store the queried data into a folder called songs_table)\n",
    "        - artists (will store the queried data into a folder called artists_table)\n",
    "    Parameters: \n",
    "        spark: Spark session object\n",
    "        input_data: path of the inputdata to be processed\n",
    "        output_data: path of the location to store the parquet files\n",
    "    Returns: \n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # get filepath to song data file\n",
    "    song_data = input_data.format(\"song-data/*/*/*/*.json\")\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data, schema=songsSchema)\n",
    "    \n",
    "    # Copy for later use\n",
    "    song_data_df = df\n",
    "    \n",
    "    # extract columns to create songs table\n",
    "    songs_table = df.select([\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\"]).dropDuplicates([\"song_id\"])\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write.mode(\"overwrite\").partitionBy(\"year\", \"artist_id\")\\\n",
    "    .parquet(output_data.format(\"songs_table\"))\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df.select([\"artist_id\",\\\n",
    "                               col(\"artist_name\").alias(\"name\"), \n",
    "                               col(\"artist_location\").alias(\"location\"),\n",
    "                               col(\"artist_latitude\").alias(\"latitude\"),\n",
    "                               col(\"artist_longitude\").alias(\"longitude\")])\\\n",
    "                    .dropDuplicates([\"artist_id\"])\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.mode(\"overwrite\")\\\n",
    "    .parquet(output_data.format(\"artists_table\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "    \"\"\"This function will read JSON input files from the provided path and extracting the data to be inserted into three tables:\n",
    "        - users (will store the queried data into a folder called users_table)\n",
    "        - time (will store the queried data into a folder called time_table)\n",
    "        - songplays (will store the queried data into a folder called songplays_table)\n",
    "    Parameters: \n",
    "        spark: Spark session object\n",
    "        input_data: path of the inputdata to be processed\n",
    "        output_data: path of the location to store the parquet files\n",
    "    Returns: \n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # get filepath to log data file\n",
    "    log_data = input_data.format(\"log-data/*.json\")\n",
    "\n",
    "    # read log data file\n",
    "    df = spark.read.json(log_data, schema=logsSchema)\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df = df[df['page'] == 'NextSong']\n",
    "\n",
    "    # extract columns for users table    \n",
    "    users_table = df.select([col(\"userId\").alias(\"user_id\"),\n",
    "                             col(\"firstName\").alias(\"first_name\"), \n",
    "                             col(\"lastName\").alias(\"last_name\"),\n",
    "                             \"gender\",\n",
    "                             \"level\"])\\\n",
    "                    .dropDuplicates([\"user_id\"])\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_table.write.mode(\"overwrite\")\\\n",
    "    .parquet(output_data.format(\"users_table\"))\n",
    "\n",
    "    # create timestamp column from original timestamp column\n",
    "    get_timestamp = udf(lambda x: datetime.fromtimestamp(x/1000.0), TimestampType())\n",
    "    df = df.withColumn(\"timestamp\", get_timestamp(df['ts']))\n",
    "    \n",
    "    # create datetime column from original timestamp column\n",
    "    get_datetime = udf(lambda x: datetime.fromtimestamp(x/1000.0), TimestampType())\n",
    "    df = df.withColumn(\"datetime\", get_datetime(df['ts']))\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table = df.select([col(\"datetime\").alias(\"start_time\"),\n",
    "                            hour(df[\"datetime\"]).alias(\"hour\"), \n",
    "                            dayofmonth(df[\"datetime\"]).alias(\"day\"), \n",
    "                            weekofyear(df[\"datetime\"]).alias(\"week\"), \n",
    "                            month(df[\"datetime\"]).alias(\"month\"), \n",
    "                            year(df[\"datetime\"]).alias(\"year\"), \n",
    "                            date_format(df[\"datetime\"], \"E\").alias(\"weekday\")])\\\n",
    "                    .dropDuplicates([\"start_time\"])\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.mode(\"overwrite\").partitionBy(\"year\", \"month\")\\\n",
    "   .parquet(output_data.format(\"time_table\"))\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    # Note: since we already read song data from the sample input data files, we will use that object instead of re-read the data again\n",
    "    song_df = song_data_df\n",
    "\n",
    "    # create songplays table \n",
    "    songplays_table = df.select([\"song\",\n",
    "                                 \"artist\",\n",
    "                                  col(\"datetime\").alias(\"start_time\"),\n",
    "                                  col(\"userId\").alias(\"user_id\"), \n",
    "                                  \"level\", \n",
    "                                  col(\"sessionId\").alias(\"session_id\"), \n",
    "                                  \"location\",\n",
    "                                 \"length\",\n",
    "                                  col(\"userAgent\").alias(\"user_agent\")])\n",
    "    # joined song and log datasets \n",
    "    songplays_table = songplays_table\\\n",
    "                      .join(song_df, (songplays_table.song == song_df.title) & \n",
    "                            (songplays_table.artist == song_df.artist_name) &\n",
    "                            (songplays_table.length == song_df.duration), 'left_outer')\n",
    "    # add a songplay_id column by auto increment this column\n",
    "    songplays_table = songplays_table.withColumn(\"songplay_id\", \\\n",
    "                                      monotonically_increasing_id())\n",
    "    # extract columns to create songplays table \n",
    "    songplays_table = songplays_table.select([\"songplay_id\", \"start_time\", \"user_id\", \"level\", \"song_id\", \"artist_id\", \"session_id\", \"location\", \"user_agent\"])\n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.withColumn(\"year\",year('start_time'))\\\n",
    "                    .withColumn(\"month\",month('start_time'))\\\n",
    "                    .write.mode(\"overwrite\")\\\n",
    "                    .partitionBy(\"year\", \"month\")\\\n",
    "                    .parquet(output_data.format(\"songplays_table\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Process the song and log data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# run this cell\n",
    "process_song_data(spark, input_data, output_data)  \n",
    "process_log_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Check the table's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read the parquet files from the songs_table folder\n",
    "songs_parquet_df = spark.read.parquet(output_data.format(\"songs_table\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "songs_parquet_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+---------+----+------------------+\n",
      "|           song_id|               title| duration|year|         artist_id|\n",
      "+------------------+--------------------+---------+----+------------------+\n",
      "|SOAOIBZ12AB01815BE|I Hold Your Hand ...| 43.36281|2000|ARPBNLO1187FB3D52F|\n",
      "|SONYPOM12A8C13B2D7|I Think My Wife I...|186.48771|2005|ARDNS031187B9924F0|\n",
      "|SODREIN12A58A7F2E5|A Whiter Shade Of...|326.00771|   0|ARLTWXK1187FB5A3F8|\n",
      "|SOYMRWW12A6D4FAB14|The Moon And I (O...| 267.7024|   0|ARKFYS91187B98E58F|\n",
      "|SOWQTQZ12A58A7B63E|Streets On Fire (...|279.97995|   0|ARPFHN61187FB575F6|\n",
      "|SOUDSGM12AC9618304|Insatiable (Instr...|266.39628|   0|ARNTLGG11E2835DDB9|\n",
      "|SOPEGZN12AB0181B3D|Get Your Head Stu...| 45.66159|   0|AREDL271187FB40F44|\n",
      "|SOOLYAZ12A6701F4A6|Laws Patrolling (...|173.66159|   0|AREBBGV1187FB523D2|\n",
      "|SOBAYLL12A8C138AF9|Sono andati? Fing...|511.16363|   0|ARDR4AC1187FB371A1|\n",
      "|SOBBUGU12A8C13E95D|Setting Fire to S...|207.77751|2004|ARMAC4T1187FB3FA4C|\n",
      "+------------------+--------------------+---------+----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show 10 records\n",
    "songs_parquet_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read the parquet files from the artists_table folder\n",
    "artists_parquet_df = spark.read.parquet(output_data.format(\"artists_table\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "artists_parquet_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+--------+---------+\n",
      "|         artist_id|                name|            location|latitude|longitude|\n",
      "+------------------+--------------------+--------------------+--------+---------+\n",
      "|ARMAC4T1187FB3FA4C|The Dillinger Esc...|   Morris Plains, NJ|40.82624|-74.47995|\n",
      "|ARNF6401187FB57032|   Sophie B. Hawkins|New York, NY [Man...|40.79086|-73.96644|\n",
      "|AROUOZZ1187B9ABE51|         Willie Bobo|New York, NY [Spa...|40.79195|-73.94512|\n",
      "|ARI2JSK1187FB496EF|Nick Ingman;Gavyn...|     London, England|51.50632| -0.12714|\n",
      "|AREBBGV1187FB523D2|Mike Jones (Featu...|         Houston, TX|    null|     null|\n",
      "|ARD842G1187B997376|          Blue Rodeo|Toronto, Ontario,...|43.64856|-79.38533|\n",
      "|AR9AWNF1187B9AB0B4|Kenny G featuring...|Seattle, Washingt...|    null|     null|\n",
      "|ARIG6O41187B988BDD|     Richard Souther|       United States|37.16793|-95.84502|\n",
      "|ARGSJW91187B9B1D6B|        JennyAnyKind|      North Carolina|35.21962|-80.01955|\n",
      "|AR3JMC51187B9AE49D|     Backstreet Boys|         Orlando, FL|28.53823|-81.37739|\n",
      "+------------------+--------------------+--------------------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show 10 records\n",
    "artists_parquet_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read the parquet files from the users_table folder\n",
    "users_parquet_df = spark.read.parquet(output_data.format(\"users_table\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "users_parquet_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------+-----+\n",
      "|user_id|first_name|last_name|gender|level|\n",
      "+-------+----------+---------+------+-----+\n",
      "|     88|  Mohammad|Rodriguez|     M| free|\n",
      "|     53|   Celeste| Williams|     F| free|\n",
      "|     75|    Joseph|Gutierrez|     M| free|\n",
      "|     60|     Devin|   Larson|     M| free|\n",
      "|     68|    Jordan|Rodriguez|     F| free|\n",
      "|     90|    Andrea|   Butler|     F| free|\n",
      "|     14|  Theodore|   Harris|     M| free|\n",
      "|      2|   Jizelle| Benjamin|     F| free|\n",
      "|     77| Magdalene|   Herman|     F| free|\n",
      "|     89|   Kynnedi|  Sanchez|     F| free|\n",
      "+-------+----------+---------+------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show 10 records\n",
    "users_parquet_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read the parquet files from the time_table folder\n",
    "time_parquet_df = spark.read.parquet(output_data.format(\"time_table\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "time_parquet_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+---+----+-------+----+-----+\n",
      "|          start_time|hour|day|week|weekday|year|month|\n",
      "+--------------------+----+---+----+-------+----+-----+\n",
      "|2018-11-02 11:33:...|  11|  2|  44|    Fri|2018|   11|\n",
      "|2018-11-02 13:30:...|  13|  2|  44|    Fri|2018|   11|\n",
      "|2018-11-02 16:46:...|  16|  2|  44|    Fri|2018|   11|\n",
      "|2018-11-04 06:26:...|   6|  4|  44|    Sun|2018|   11|\n",
      "|2018-11-04 20:09:...|  20|  4|  44|    Sun|2018|   11|\n",
      "|2018-11-05 12:26:...|  12|  5|  45|    Mon|2018|   11|\n",
      "|2018-11-05 14:39:...|  14|  5|  45|    Mon|2018|   11|\n",
      "|2018-11-06 21:12:...|  21|  6|  45|    Tue|2018|   11|\n",
      "|2018-11-06 23:28:...|  23|  6|  45|    Tue|2018|   11|\n",
      "|2018-11-07 14:16:...|  14|  7|  45|    Wed|2018|   11|\n",
      "+--------------------+----+---+----+-------+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show 10 records\n",
    "time_parquet_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read the parquet files from the songplays_table folder\n",
    "songplays_parquet_df = spark.read.parquet(output_data.format(\"songplays_table\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- songplay_id: long (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- session_id: long (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- user_agent: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "songplays_parquet_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------+-----+-------+---------+----------+--------------------+--------------------+----+-----+\n",
      "|songplay_id|          start_time|user_id|level|song_id|artist_id|session_id|            location|          user_agent|year|month|\n",
      "+-----------+--------------------+-------+-----+-------+---------+----------+--------------------+--------------------+----+-----+\n",
      "|          0|2018-11-15 00:30:...|     26| free|   null|     null|       583|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|2018|   11|\n",
      "|          1|2018-11-15 00:41:...|     26| free|   null|     null|       583|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|2018|   11|\n",
      "|          2|2018-11-15 00:45:...|     26| free|   null|     null|       583|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|2018|   11|\n",
      "|          3|2018-11-15 03:44:...|     61| free|   null|     null|       597|Houston-The Woodl...|\"Mozilla/5.0 (Mac...|2018|   11|\n",
      "|          4|2018-11-15 05:48:...|     80| paid|   null|     null|       602|Portland-South Po...|\"Mozilla/5.0 (Mac...|2018|   11|\n",
      "|          5|2018-11-15 05:53:...|     80| paid|   null|     null|       602|Portland-South Po...|\"Mozilla/5.0 (Mac...|2018|   11|\n",
      "|          6|2018-11-15 05:55:...|     80| paid|   null|     null|       602|Portland-South Po...|\"Mozilla/5.0 (Mac...|2018|   11|\n",
      "|          7|2018-11-15 06:01:...|     80| paid|   null|     null|       602|Portland-South Po...|\"Mozilla/5.0 (Mac...|2018|   11|\n",
      "|          8|2018-11-15 06:07:...|     80| paid|   null|     null|       602|Portland-South Po...|\"Mozilla/5.0 (Mac...|2018|   11|\n",
      "|          9|2018-11-15 06:10:...|     80| paid|   null|     null|       602|Portland-South Po...|\"Mozilla/5.0 (Mac...|2018|   11|\n",
      "+-----------+--------------------+-------+-----+-------+---------+----------+--------------------+--------------------+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show 10 records\n",
    "songplays_parquet_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
